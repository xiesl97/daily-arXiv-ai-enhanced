<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.CL](#cs.CL) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?](https://arxiv.org/abs/2511.20710)
*David Amebley,Sayanton Dibbo*

Main category: cs.CV

TL;DR: 神经科学启发的拓扑正则化（tau框架）提升VLMs对成员推断攻击（MIA）的鲁棒性24%，不显著牺牲模型效用。


<details>
  <summary>Details</summary>
Motivation: 多模态模型部署增多导致隐私泄露风险上升，现研究多聚焦单模态系统或对抗攻击，神经启发多模态模型对隐私攻击鲁棒性未探索。

Method: 提出系统性神经科学启发拓扑正则化tau框架，应用于BLIP、PaliGemma 2、ViT-GPT2等VLMs（NEURO变体tau&gt;0），在COCO、CC3M、NoCaps数据集上评估黑盒MIA攻击与模型效用（MPNet、ROUGE-2）。

Result: BLIP+COCO数据集上，NEURO VLM MIA成功率（ROC-AUC）平均下降24%，效用相似；PaliGemma 2与ViT-GPT2在CC3M、NoCaps上结果一致。

Conclusion: 神经启发VLMs对图像-文本MIA隐私攻击更具鲁棒性，提供多模态模型隐私风险理解新证据。

Abstract: In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.

</details>


### [2] [Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation](https://arxiv.org/abs/2511.20714)
*Inferix Team,Tianyu Feng,Yizeng Han,Jiahao He,Yuanyu He,Xi Lin,Teng Liu,Hanfeng Lu,Jiasheng Tang,Wei Wang,Zhiyuan Wang,Jichao Wu,Mingyang Yang,Yinghao Yu,Zeyu Zhang,Bohan Zhuang*

Main category: cs.CV

TL;DR: Inferix is a next-generation inference engine for world models using semi-autoregressive decoding to generate high-quality, interactive videos efficiently.


<details>
  <summary>Details</summary>
Motivation: World models need efficient inference for long, realistic videos in AI agents, embodied AI, gaming; semi-autoregressive decoding overcomes limitations of diffusion and autoregressive methods; need for optimized engine beyond high-concurrency or classic diffusion systems.

Method: Semi-autoregressive (block-diffusion) decoding: generates video tokens in blocks with diffusion within blocks, autoregressive across blocks, using LLM-style KV Cache; Inferix optimizes this with interactive streaming, profiling, and LV-Bench integration.

Result: Enables efficient, variable-length, high-quality video generation; supports real-time interaction, realistic simulation; provides fine-grained benchmarking via LV-Bench.

Conclusion: Inferix advances world model inference, distinct from existing systems; calls for community collaboration to foster exploration.

Abstract: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.

</details>


### [3] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU,Lianming Huang,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: DeeAD是一种无训练、动作引导的早退框架，通过评估中间轨迹的物理可行性加速VLA规划，当预测轨迹与轻量规划先验偏差小于2m时提前终止推理，实现高达29%的延迟降低。


<details>
  <summary>Details</summary>
Motivation: VLA模型统一了感知、推理和轨迹生成，但深层Transformer栈导致显著推理延迟，影响自动驾驶实时性。

Method: 使用动作引导早退而非置信分数，当轨迹与导航或低精度规划先验对齐（偏差<2m）时终止；引入多跳控制器根据分数变化率自适应跳过冗余层；无缝集成现有VLA模型如ORION，无需重训练。

Result: 在Bench2Drive基准上，实现28% Transformer层稀疏度和29%延迟降低，同时保持规划质量和安全性。

Conclusion: DeeAD有效加速VLA推理，证明了在不牺牲性能前提下显著提升效率的可行性。

Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.

</details>


### [4] [DinoLizer: Learning from the Best for Generative Inpainting Localization](https://arxiv.org/abs/2511.20722)
*Minh Thong Doi,Jan Butora,Vincent Itier,Jérémie Boulanger,Patrick Bas*

Main category: cs.CV

TL;DR: 基于DINOv2的DinoLizer模型，用于生成式修复图像中操纵区域的定位，超越现有最佳方法，在多种数据集上IoU提升12%，对后处理鲁棒。


<details>
  <summary>Details</summary>
Motivation: 生成式修复图像中定位语义操纵区域的需求，区分语义更改与非语义编辑，利用DINOv2的强大表示能力。

Method: 在B-Free数据集预训练的DINOv2基础上添加线性分类头，对ViT的14×14补丁嵌入预测操纵；滑动窗口处理大图，热图后处理生成二值掩码；训练聚焦语义更改区域。

Result: 在多种生成模型的修复数据集上超越SOTA，平均IoU高12%，后处理（如缩放、噪声、JPEG压缩）后优势更大；DINOv2优于DINOv3的消融验证。

Conclusion: Vision Transformer在该任务中表现出色，DinoLizer性能优越，代码即将公开。

Abstract: We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.

</details>


### [5] [CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design](https://arxiv.org/abs/2511.20737)
*Daeheon Jeong,Seoyeon Byun,Kihoon Son,Dae Hyun Kim,Juho Kim*

Main category: cs.CV

TL;DR: 本文提出CANVAS基准，用于评估视觉语言模型（VLM）在基于工具的用户界面（UI）设计任务中的能力。基准包含598个从3.3K移动UI设计中采样的任务，覆盖30个功能类别，包括设计复制和设计修改两类任务。VLM通过上下文工具调用逐步编辑设计。实验显示领先模型采用策略性工具调用提升设计质量，并识别常见错误模式。


<details>
  <summary>Details</summary>
Motivation: UI设计是迭代过程，设计师使用Figma等软件逐步精炼设计。VLMs的工具调用能力表明其可操作设计软件与设计师协作，但缺乏评估工具设计性能的基准，导致能力未知。开发此类基准有助于揭示VLMs在传统软件中协作潜力。

Method: 引入CANVAS基准，包含598个工具设计任务及 ground-truth 参考，从30功能类别（例如 onboarding、messaging）的3.3K移动UI设计中采样。每任务中，VLM通过上下文工具调用（如创建矩形作为按钮背景）逐步更新设计，与设计软件链接。任务分为两类：(i) 设计复制（重现整个UI屏幕）；(ii) 设计修改（修改现有屏幕特定部分）。

Result: 领先模型表现出更策略性的工具调用，从而改善设计质量。识别模型常见错误模式，为未来增强工具设计能力提供指导。

Conclusion: CANVAS基准揭示VLMs在工具UI设计中的能力，强调其与设计师协作潜力，并通过错误分析指导未来改进。

Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.

</details>


### [6] [Text-Guided Semantic Image Encoder](https://arxiv.org/abs/2511.20770)
*Raghuveer Thirukovalluru,Xiaochuang Han,Bhuwan Dhingra,Emily Dinan,Maha Elbayad*

Main category: cs.CV

TL;DR: 提出Text-Guided Semantic Image Encoder (TIE)，一种受文本查询引导的图像编码器，用于视觉语言模型（VLMs），提升图像到文本基准性能并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统图像编码器独立预训练，对下游任务或文本查询无感知，导致图像处理缺乏针对性。

Method: 提出TIE，通过输入文本查询条件化生成图像表示，提升编码器对查询相关视觉特征的捕捉。

Result: 在9个图像到文本基准上，1B和3B规模平均提升1.5和1.3分，DocVQA和InfoVQA最高达6分；仅用一半图像瓦片（tokens），推理效率显著提升。

Conclusion: TIE对通用查询泛化良好，定性分析显示其关注查询相关区域，提高可解释性和查询特定 grounding。

Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.

</details>


### [7] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: 提出Δ-NeRF，一种模块化残差框架，用于NeRF的增量精炼，无需过去数据即可避免灾难性遗忘，支持卫星地形分析等顺序数据场景。通过残差控制器、不确定性门控、视图选择（减少47%数据）和知识蒸馏（压缩至20%大小），实现高效精炼。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF在引入新视图时需完整重训，限制了顺序数据场景（如卫星反复观测地形）的应用；增量精炼研究不足，朴素方法易灾难性遗忘。

Method: Δ-NeRF框架：(1) 残差控制器向冻结基NeRF的每层注入修正；(2) 不确定性感知门控机制自适应融合基预测与精炼预测，避免过度修正；(3) 视图选择策略减少训练数据47%；附加知识蒸馏压缩模型至原20%大小。

Result: 卫星图像实验显示，Δ-NeRF性能媲美联合训练，训练时间减少30-42%；优于基线，PSNR较朴素微调提升至43.5%，某些指标超联合训练。

Conclusion: Δ-NeRF提供无需过去数据的高效NeRF增量精炼方案，在顺序数据领域如卫星分析中表现出色，显著降低时间与资源消耗。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [8] [Layer-Aware Video Composition via Split-then-Merge](https://arxiv.org/abs/2511.20809)
*Ozgur Kara,Yujia Chen,Ming-Hsuan Yang,James M. Rehg,Wen-Sheng Chu,Du Tran*

Main category: cs.CV

TL;DR: 提出Split-then-Merge (StM)框架，用于提升生成视频组合的控制性并解决数据稀缺问题。通过将无标签视频拆分为动态前景和背景层，并自组合学习动态主体与场景交互，实现真实视频生成。


<details>
  <summary>Details</summary>
Motivation: 生成视频组合缺乏控制，且依赖标注数据集或手工规则；StM利用无标签视频大规模自组合，学习复杂组合动态，解决数据稀缺。

Method: 将无标签视频拆分为动态前景/背景层，自组合生成训练数据；引入变换感知训练管道，包括多层融合与增强实现可供性感知组合，以及身份保持损失确保前景保真。

Result: 在定量基准和人类/VLLM定性评估中优于SOTA方法。

Conclusion: StM有效提升生成视频组合性能，项目页面详见https://split-then-merge.github.io。

Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io

</details>


### [9] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam,Saksham Aggarwal,Justin Yang Chae,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Sphinx是一个用于视觉感知和推理的合成环境基准，涵盖25种任务类型，生成带有可验证ground-truth的谜题。顶级LVLMs如GPT-5准确率仅51.1%，远低于人类；RLVR显著提升性能，并在外部基准上获益。


<details>
  <summary>Details</summary>
Motivation: 针对视觉语言模型（LVLMs）在核心认知原语（如对称、几何变换、空间推理等）上的评估需求，现有基准缺乏精确性和大规模性，现引入Sphinx以提供可验证解决方案和大规模数据集。

Method: Sphinx使用图案、图块、图表、图标和几何原语过程化生成谜题，覆盖25种任务（对称检测、几何变换、空间推理、图表解读、序列预测），配以ground-truth；评估LVLMs，并使用可验证奖励的强化学习（RLVR）训练模型。

Result: 顶级LVLMs如GPT-5在Sphinx上准确率仅51.1%，远低于人类水平；RLVR显著提升模型在Sphinx任务上的准确率，并在外部视觉推理基准上产生正迁移。

Conclusion: Sphinx突显LVLMs在基础视觉推理上的不足，RLVR通过可验证奖励展示出推进多模态推理的潜力。

Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.

</details>


### [10] [RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs](https://arxiv.org/abs/2511.20823)
*Roman Naeem,David Hagerman,Jennifer Alvén,Fredrik Kahl*

Main category: cs.CV

TL;DR: RefTr是一种3D图像到图模型，用于血管树中心线生成，通过Transformer解码器的Producer-Refiner架构，对初始汇合轨迹进行递归精炼，实现高召回率和高效拓扑正确的树状中心线提取。


<details>
  <summary>Details</summary>
Motivation: 管状树结构（如血管和肺气道）对人体物质运输至关重要，准确检测其中心线并保持正确树拓扑对诊断、治疗规划和手术导航等临床任务关键，高召回率尤为重要，避免遗漏小分支导致致命错误。

Method: 采用基于Transformer解码器的Producer-Refiner架构：Producer提出初始汇合轨迹集，Refiner递归精炼产生最终轨迹形成中心线图；汇合轨迹表示确保完整轨迹精炼并强制有效树拓扑；引入高效空间树图非最大抑制算法合并重复分支；递归精炼提升精度并减少2.4倍解码器参数。

Result: 在多个公开中心线数据集上，RefTr实现优于SOTA的召回率、相当精度、更快推理速度和显著更少参数。

Conclusion: RefTr作为血管树3D医学影像分析的新SOTA框架，具有巨大潜力。

Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.

</details>


### [11] [MODEST: Multi-Optics Depth-of-Field Stereo Dataset](https://arxiv.org/abs/2511.20853)
*Nisarg K. Trivedi,Vinayak A. Belludi,Li-Yun Wang,Pardis Taghavi,Dante Lok*

Main category: cs.CV

TL;DR: 提出首个高分辨率（5472×3648px）真实立体DSLR数据集，包含18000张图像，系统变焦距（28-70mm）和光圈（f/2.8-f/22），覆盖9个复杂真实场景，用于深度估计、景深渲染等任务评估与泛化研究。


<details>
  <summary>Details</summary>
Motivation: 真实光学条件下可靠深度估计仍是核心挑战，现研究受限于缺乏大规模、高保真真实立体DSLR数据集，导致合成数据训练模型真实世界泛化与评估困难。

Method: 针对9个复杂度、照明、背景各异的场景，使用两个相同相机组件，在10个焦距和5个光圈下捕获每场景2000张图像（总18000张），全覆盖50种光学配置；每个配置附专用校准图像集；场景包含多尺度光学错觉、反射/透明表面、细粒细节及光照变化等挑战元素。

Result: 发布数据集、校准文件及评估代码；展示当前单目/立体深度估计、景深渲染等SOTA方法在真实光学下的挑战，支持几何/光学效应分析、去模糊、3D重建及新视图合成等。

Conclusion: 桥接合成训练数据与真实相机光学间的现实差距，推动可复现的真实世界光学泛化研究。

Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.

</details>


### [12] [Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries](https://arxiv.org/abs/2511.20854)
*Sree Bhattacharyya,Yaman Kumar Singla,Sudhir Yarram,Somesh Kumar Singh,Harini S,James Z. Wang*

Main category: cs.CV

TL;DR: 引入首个大规模无监督视觉记忆数据集，含超82,000个视频及Reddit ToT查询的回忆描述，支持回忆生成和ToT检索任务，微调VLMs超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 视觉内容记忆性研究因人类标注成本高而数据集规模和多样性受限，现有的记忆分数数据集无法捕捉开放回忆描述中的细粒度信号。

Method: 利用Reddit等平台的ToT检索查询收集无监督回忆数据，构建大规模视频数据集；针对回忆生成任务微调大型视觉语言模型；采用对比训练策略开发多模态ToT检索模型。

Result: 微调模型在生成开放式记忆描述上优于GPT-4o等SOTA模型；成功创建首个支持多模态ToT检索的模型，该数据集为相关任务提供丰富信号。

Conclusion: 该数据集和模型开辟视觉内容记忆性研究新方向，促进人类记忆理解和内容设计应用的进步。

Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.

</details>


### [13] [Estimating Fog Parameters from a Sequence of Stereo Images](https://arxiv.org/abs/2511.20865)
*Yining Ding,João F. C. Mota,Andrew M. Wallace,Sen Wang*

Main category: cs.CV

TL;DR: 提出一种从立体雾天图像序列同时估计雾模型参数并动态更新的方法，与顺序估计方法不同，避免误差传播。假设雾局部均匀，有效处理真实非均匀雾，可集成到SLAM或里程计系统中。新数据集SDIRF包含40分钟34k帧真实雾天驾驶立体图像及晴天对应数据。实验证明优于现有方法，发布代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法顺序估计雾参数易导致误差传播，且假设雾全局均匀，无法有效处理真实世界中常见的非均匀雾。雾天环境下视觉SLAM或里程计系统需要准确的雾参数估计，以提升鲁棒性。

Method: 通过求解新型优化问题，同时估计所有雾模型参数（动态更新），仅假设雾局部均匀，避免全局均匀假设。易作为插件集成到现有SLAM/里程计系统中。

Result: 在合成雾数据和真实SDIRF数据集上，参数估计最准确，适应真实雾效果更好，优于先前方法。

Conclusion: 提出方法有效提升雾天视觉感知性能，提供新数据集SDIRF和代码，促进相关研究。

Abstract: We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.

</details>


### [14] [V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence](https://arxiv.org/abs/2511.20886)
*Jiancheng Pan,Runze Wang,Tianwen Qian,Mohammad Mahdi,Yanwei Fu,Xiangyang Xue,Xiaomeng Huang,Luc Van Gool,Danda Pani Paudel,Yuqian Fu*

Main category: cs.CV

TL;DR: V^2-SAM 通过两个互补提示生成器（V^2-Anchor 和 V^2-Visual）和后验循环一致性选择器（PCCS），将 SAM2 从单视图分割适配到跨视图物体对应，在多个基准上取得 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 跨视图物体对应（如自我-外部视图）由于视角和外观剧变而极具挑战，现成分割模型如 SAM2 难以直接应用，需要统一的跨视图框架。

Method: V^2-SAM 框架包括：基于 DINOv3 特征的跨视图锚点提示生成器（V^2-Anchor），建立几何感知对应并首次启用 SAM2 的坐标提示；跨视图视觉提示生成器（V^2-Visual），通过新型视觉提示匹配器从特征和结构视角增强外观引导。多专家设计结合后验循环一致性选择器（PCCS）自适应选择最佳专家。

Result: 在 Ego-Exo4D（自我-外部物体对应）、DAVIS-2017（视频物体跟踪）和 HANDAL-X（机器人跨视图对应）数据集上实现新的最先进性能。

Conclusion: V^2-SAM 有效解决了跨视图物体对应挑战，大量实验证实其优越性。

Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).

</details>


### [15] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出Null-Text Test-Time Alignment (Null-TTA)，通过优化classifier-free guidance中的无条件嵌入，实现扩散模型的测试时对齐，避免欠优化或奖励黑客攻击，直接引导生成分布向目标奖励对齐，无需更新模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法易导致欠优化或过优化（奖励黑客），因其操纵潜在或噪声变量，脱离语义流形；需一种在语义连贯流形上对齐的方法。

Method: 优化classifier-free guidance中的无条件嵌入，利用文本嵌入空间的结构化语义性质，确保对齐发生在语义连贯流形上，直接转向模型生成分布。

Result: 在目标测试时对齐上达到SOTA性能，同时保持强跨奖励泛化。

Conclusion: 确立语义空间优化作为TTA的有效且有原则的新范式。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology](https://arxiv.org/abs/2511.20665)
*Tcharlies Schmitz*

Main category: cs.CL

TL;DR: Introduces Harmonic Token Projection (HTP), a reversible and deterministic framework for training-free text embeddings using harmonic trajectories derived from Unicode integers, achieving Spearman ρ=0.68 on STS-B English with stable multilingual performance and minimal compute.


<details>
  <summary>Details</summary>
Motivation: Motivated by the limitations of neural embeddings that depend on statistical co-occurrence, optimization, training data, vocabularies, and stochastic parameters, HTP aims to create bijective, interpretable mappings from discrete tokens to continuous vectors purely analytically via harmonic geometry.

Method: HTP encodes each token's Unicode integer representation analytically into a harmonic trajectory, providing phase-coherent projections that are bijective, reversible, and preserve structure; semantic similarity is computed from geometric alignment without any training or stochasticity.

Result: On Semantic Textual Similarity Benchmark (STS-B), achieves Spearman correlation ρ=0.68 in English, maintains stable performance across ten languages, with negligible computational cost and sub-millisecond latency per sentence pair.

Conclusion: Proves that meaningful semantic relations can emerge from deterministic geometric structures alone, positioning HTP as a transparent, efficient, multilingual alternative to data-driven embeddings.

Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.

</details>


### [17] [PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation](https://arxiv.org/abs/2511.20668)
*Yongfu Xue*

Main category: cs.CL

TL;DR: PIRA：一种针对LLM奖励模型的新训练范式，通过偏好指令重构、多样任务奖励聚合和变dropout值头平均，解决数据效率低和过优化问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对齐LLM与人类偏好至关重要，但传统判别式模型存在两大挑战：(1) 直接拼接问答输入导致数据效率低；(2) 易受奖励过优化影响。

Method: 提出PIRA训练范式，包括三策略：(1) 将问答对重构为明确偏好指令；(2) 聚合多样偏好任务奖励以减偏并提升鲁棒性；(3) 在不同dropout率下平均值头输出以稳定奖励。

Result: 广泛实验验证了PIRA的有效性。

Conclusion: PIRA有效解决了传统奖励模型的挑战，提升了对齐性能。

Abstract: Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.

</details>


### [18] [Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data](https://arxiv.org/abs/2511.20669)
*Mann Khatri,Mirza Yusuf,Rajiv Ratn Shah,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在法律任务中表现欠佳，本文通过三种零样本上下文学习方法改善：在三个印度法律判决预测数据集上实验，包括按修辞角色重组文档、定义修辞角色以熟悉法律术语，以及模拟法院逐步推理，F1分数提升1.5%~4.36%。


<details>
  <summary>Details</summary>
Motivation: LLMs虽具备通用推理能力，但缺乏法律领域特定预训练，且法律文档冗长复杂，难以高效处理全文；以往研究使用上下文方法填补知识缺口，本文针对法律任务分析模型行为。

Method: 零样本设置下，在三个印度法律判决预测数据集上开展三类实验：(i) 按修辞角色重组文档，评估结构化信息对长上下文处理和决策的影响；(ii) 定义修辞角色，帮助模型熟悉法律术语；(iii) 模拟法院对修辞角色的逐步推理，提升模型推理能力。

Result: 组织数据或解释关键法律术语显著提升性能，F1分数相比基线最低提升约1.5%，最高4.36%。

Conclusion: 上下文方法如文档重组、术语解释和推理模拟有效提升LLMs在法律任务中的表现，无需完整领域对齐。

Abstract: Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.

</details>


### [19] [MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data](https://arxiv.org/abs/2511.20672)
*Saad Mankarious,Ayah Zirikly,Daniel Wiechmann,Elma Kerz,Edward Kempa,Yu Qiao*

Main category: cs.CL

TL;DR: New Reddit benchmark dataset MindSET with over 13M annotated posts across 7 mental health conditions, rigorously preprocessed (language filter, NSFW/duplicate removal), LIWC analysis, outperforms prior benchmarks by up to 18 F1 points in diagnosis detection.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for social media mental health analysis are outdated due to limited data, inadequate cleaning, and content diversity (multilingual, harmful material), missing real-time insights.

Method: Curated from Reddit using self-reported diagnoses; annotated 13M+ posts; preprocessing: language filtering, NSFW/duplicate removal; LIWC linguistic analysis; binary classification with fine-tuned LMs and BoW features.

Result: Models on MindSET outperform prior benchmarks, achieving up to 18-point F1 improvement for Autism detection.

Conclusion: MindSET provides a robust foundation for mental health research from social media, supporting early risk detection and psychological trend analysis.

Abstract: Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.

</details>


### [20] [Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation](https://arxiv.org/abs/2511.20673)
*Zheng Hui,Xiaokai Wei,Reza Shirkavand,Chen Wang,Weizhi Zhang,Alejandro Peláez,Michelle Gong*

Main category: cs.CL

TL;DR: 生成推荐新范式FlexCode，通过流行度感知的自适应码本分配（CF码本与语义码本），平衡热门与长尾物品表示，提升准确性和尾部鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一统一码本编码所有物品，忽略热门物品（协作信号丰富）与长尾物品（依赖语义理解）的失衡，导致表示效率低和泛化能力差。

Method: 提出FlexCode框架：流行度感知，自适应分配固定令牌预算至协作过滤（CF）码本和语义码本；轻量MoE动态平衡CF精度与语义泛化；对齐与平滑目标确保跨流行度谱系的一致性。

Result: 在公共和工业规模数据集上实验，FlexCode持续优于强基线，实现更高准确率和长尾鲁棒性。

Conclusion: FlexCode为生成推荐器提供令牌表示新机制，平衡记忆与泛化，并带来对基于令牌推荐模型的新视角。

Abstract: Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.

</details>


### [21] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed,May Alsofyani,Saad Almohaimeed,Mansour Al Ghanim,Liqiang Wang*

Main category: cs.CL

TL;DR: 本文引入首个阿拉伯语跨域、上下文依赖的text-to-SQL数据集Ar-SParC，包含3450个序列（总10225个问题）。使用GPT-3.5-turbo和GPT-4.5-turbo进行40个实验，测试10种提示工程技术（4种问题表示法+6种上下文学习）。提出新型GAT corrector，提升零样本EX/IX准确率平均1.9%，上下文学习下EX 1.72%、IX 0.92%。消融实验解释其优于GAT verifier的原因，尤其针对阿拉伯语。


<details>
  <summary>Details</summary>
Motivation: 现有跨域、上下文依赖text-to-SQL数据集和研究主要限于英语及少量中文，阿拉伯语领域尚无相关工作。本文旨在填补这一空白，推动阿拉伯语自然语言与数据库交互。

Method: 构建Ar-SParC数据集：3450序列、平均3问、总10225问及对应SQL。开展40实验：2个大模型（GPT-3.5-turbo、GPT-4.5-turbo）、10种提示技术（4问题表示+6 in-context learning）。开发GAT corrector进行SQL修正，进行2个消融实验对比GAT verifier。

Result: GAT corrector在所有40实验中提升性能：零样本平均EX +1.9%、IX +1.9%；in-context学习EX +1.72%、IX +0.92%。消融显示其在阿拉伯语上优于GAT verifier。

Conclusion: Ar-SParC为阿拉伯语text-to-SQL提供首个数据集，GAT corrector显著提升性能，消融研究阐明其优势，特别是阿拉伯语特定挑战。

Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.

</details>


### [22] [Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes](https://arxiv.org/abs/2511.20680)
*Matthew W. Kenaston,Umair Ayub,Mihir Parmar,Muhammad Umair Anjum,Syed Arsalan Ahmed Naqvi,Priya Kumar,Samarth Rawal,Aadel A. Chaudhuri,Yousef Zakharia,Elizabeth I. Heath,Tanios S. Bekaii-Saab,Cui Tao,Eliezer M. Van Allen,Ben Zhou,YooJung Choi,Chitta Baral,Irbaz Bin Riaz*

Main category: cs.CL

TL;DR: 大型语言模型在肿瘤学临床基准上表现优异，但可能通过错误推理得出正确结论，此安全隐患未被准确率评估捕捉。本研究开发推理错误分层分类法，并验证其临床相关性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽基准性能高，但推理故障可能导致肿瘤决策支持中的临床不安全推荐，需开发捕捉推理失败的评估框架。

Method: 两队列回顾性研究：使用CORAL数据集乳腺癌和胰腺癌笔记，标注GPT-4思维链600条响应，定义三级分类法映射计算失败至认知偏差框架；在822条前列腺癌咨询笔记响应上验证，模拟提取、分析和临床推荐任务。

Result: 23%解释存在推理错误，主导整体错误，以确认偏差和锚定偏差最常见；推理失败与指南不一致及潜在有害推荐相关，尤其晚期疾病管理；先进语言模型自动化评估器能检测错误存在，但无法可靠分类子类型。

Conclusion: 大型语言模型推理瑕疵可能产生流利但临床不安全的推荐；所提分类法为临床部署前评估和提升推理保真度提供通用框架。

Abstract: Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.

</details>


### [23] [Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches](https://arxiv.org/abs/2511.20683)
*Bharadwaj Yadavalli*

Main category: cs.CL

TL;DR: 提出动态模板选择（DTS），根据查询复杂度自适应选择响应模板，显著降低令牌成本而不影响响应质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对各种查询使用统一冗长响应策略，导致令牌低效，尤其是输出令牌价格为输入的4-8倍。

Method: 开发DTS，使用简单MLP（基于预计算嵌入）或微调RoBERTa作为路由器，根据查询复杂度选择模板；在1000个MMLU问题上评估，并通过9000个生产API调用跨OpenAI GPT-4、Google Gemini和Anthropic Claude验证。

Result: MLP路由准确率达90.5%，略高于RoBERTa（89.5%），参数少1.25亿；跨提供商准确率一致，令牌减少32.6%-33.9%。

Conclusion: 贡献包括：机器学习理论基础下的形式化问题表述、四个算法及其复杂度分析、生产系统中的广泛实证验证。

Abstract: Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.
  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.
  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.

</details>


### [24] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang,Yadong Yu,Wenqiang Kang,Jian Zhou,Dongyue Gao,Pan Xiang,Zhe Liu,Mengyan Dai,Zhonglu Guo,Zhimei Sun*

Main category: cs.CL

TL;DR: 二维（2D）材料因独特物理化学和电子性能在能源存储与转换领域应用广泛，其属性和制备信息主要散布于研究论文中，但合成信息分散导致获取困难。


<details>
  <summary>Details</summary>
Motivation: 解决2D材料相关宝贵信息（如属性和制备方法）散布于海量论文中、难以高效提取和整合的问题。

Method: 摘要未完整提供具体方法，初步提及文献中信息的分散性，可能涉及文本挖掘或数据库构建（推测）。

Result: 摘要未提供实验结果。

Conclusion: 摘要未提供结论。

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>


### [25] [Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models](https://arxiv.org/abs/2511.20799)
*Trung Cuong Dang,David Mohaisen*

Main category: cs.CL

TL;DR: 大语言模型易于逐字记忆训练数据，引发隐私和版权风险。现有多项记忆定义存在缺陷，尤其是对齐模型。本文提出多前缀记忆框架：记忆序列可通过大量不同前缀检索。形式化定义为对抗搜索能找到目标数量不同前缀引发序列。通过开源和对齐聊天模型实验，证明其可靠区分记忆与非记忆数据，为审计LLM数据泄漏提供工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练于海量语料，易逐字记忆训练数据，造成隐私和版权风险。现有记忆定义无法全面捕捉现象，尤其在对齐模型中表现欠佳，需要新型框架全面衡量记忆鲁棒性。

Method: 引入多前缀记忆框架，核心洞见：记忆序列深度编码，可通过显著更多不同前缀检索。形式化：序列为记忆，若外部对抗搜索能识别目标数量不同前缀引发它。将焦点从单路径提取转向量化记忆鲁棒性，即检索路径多样性。

Result: 在开源模型和对齐聊天模型上的实验显示，多前缀定义可靠区分记忆与非记忆数据。

Conclusion: 多前缀记忆框架为审计LLM数据泄漏提供鲁棒、实用的工具。

Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.

</details>


### [26] [Structured Prompting Enables More Robust, Holistic Evaluation of Language Models](https://arxiv.org/abs/2511.20836)
*Asad Aali,Muhammad Ahmed Mohsin,Vasiliki Bikia,Arnav Singhvi,Richard Gaus,Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Yifan Mai,Jordan Cahoon,Michael Pfeffer,Roxana Daneshjou,Sanmi Koyejo,Emily Alsentzer,Percy Liang,Christopher Potts,Nigam H. Shah,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 提出DSPy+HELM框架，通过结构化提示方法提升语言模型（LM）基准测试准确性，发现HELM低估LM性能4%，开源整合代码。


<details>
  <summary>Details</summary>
Motivation: 现有基准如HELM依赖固定提示，无法泛化到不同LM，导致性能低估，无法准确估计LM上限，影响部署决策。

Method: 开发可复现DSPy+HELM框架，引入4种结构化提示（包括链式思考），在7个通用/医疗基准上评估4个前沿LM，与HELM基线比较。

Result: 无结构提示时：(i) HELM低估性能4%；(ii) 基准间变异增加2%标准差；(iii) 排行在3/7基准翻转；(iv) 推理提示降低LM对提示设计敏感性。

Conclusion: 首次大规模实证刻画LM在基准与提示方法间的行为，证明可扩展上限估计提供更可靠基准，并开源DSPy+HELM整合与提示优化管道。

Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller Δ across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

</details>


### [27] [Length-MAX Tokenizer for Language Models](https://arxiv.org/abs/2511.20849)
*Dong Dong,Weijie Su*

Main category: cs.CL

TL;DR: 提出Length-MAX分词器，通过最小化平均每字符token数，减少语言模型训练和推理中的token数量，比BPE减少14-18% token，实现训练加速、下游任务提升和推理效率优化。


<details>
  <summary>Details</summary>
Motivation: 现有分词器如BPE仅基于频率优化，未考虑token长度，导致平均token较长，增加token数量和计算开销；动机是通过长度加权优化，缩短平均token长度以提高语言模型效率。

Method: 将长度加权目标最大化问题转化为图分割问题，并开发贪婪近似算法，从10K至64K词汇规模构建词汇表。

Result: 在FineWeb及多领域数据集上，减少14-18% token（64K时13%）；从头训练GPT-2（124M/355M/1.3B）需少18.5%/17.2%/18.5%步达固定损失，推理延迟降13.7%/12.7%/13.7%，124M时吞吐增16%；下游LAMBADA困惑度降11.7%，HellaSwag准确率升4.3%；词汇覆盖99.62%，OOV率0.12%；推理嵌入和KV-cache内存降18%。

Conclusion: 优化平均token长度而非仅频率，是高效语言建模的有效途径，不牺牲且常改善下游性能；兼容生产系统，显著降低内存和延迟。

Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

</details>
