<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: VLMs缺乏空间智能鲁棒性，提出G²VLM，通过学习3D视觉几何特征桥接3D重建与空间理解，提升空间推理。


<details>
  <summary>Details</summary>
Motivation: VLMs在空间理解和推理任务上表现差，归因于缺少从2D图像重建3D空间的视觉几何学习过程。

Method: G²VLM是一个几何 grounding 的视觉语言模型，利用学习到的3D视觉几何特征直接预测3D属性，并通过in-context learning和交错推理增强空间推理；在多视图图像和视频数据上训练，同时利用无需硬收集标注的3D视觉先验。

Result: 在3D重建任务上与SOTA前馈模型相当，在空间理解和推理任务上更好或竞争力。

Conclusion: 统一语义强的VLM与低级3D视觉任务，希望作为社区强基线，开启3D场景编辑等未来应用。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [2] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出一种新型系统，通过少样本学习实现用户对AI误分类的低资源纠正，结合服务器端知识蒸馏和设备端原型分类，仅需原型更新而非模型重训。


<details>
  <summary>Details</summary>
Motivation: 日常设备AI模型预测错误损害用户体验，现方案多限于错误检测，缺乏资源受限设备的高效纠正机制。

Method: 服务器端：知识蒸馏将基础模型特征转移至设备架构；设备端：基于原型的分类，通过原型适应高效纠错。适用于图像分类和目标检测。

Result: Food-101和Flowers-102数据集上一-shot纠错率超50%，遗忘率<0.02%，计算开销极低；Android app实证实用性。

Conclusion: 系统高效实用，显著提升设备端AI纠错能力，适用于真实场景。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [3] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: 提出ADVLA框架，直接在视觉编码器投影到文本特征空间的特征上施加低幅度对抗扰动，高效破坏VLA模型下游动作预测，扰动专注稀疏，几乎不可察觉。


<details>
  <summary>Details</summary>
Motivation: 现有VLA对抗攻击方法需昂贵端到端训练，且产生明显扰动补丁，限制实用性。

Method: ADVLA框架在视觉编码器投影特征上直接施加扰动至文本空间，利用注意力指导实现扰动专注与稀疏；引入三种策略：提升敏感性、强制稀疏、集中扰动。

Result: L∞=4/255约束下，结合Top-K掩码修改&lt;10%补丁，攻击成功率近100%；扰动集中关键区域，几乎不可察觉；单步迭代仅0.06秒，远超传统补丁攻击。

Conclusion: ADVLA在低幅度、局部稀疏条件下有效削弱VLA动作预测，避免高训练成本与明显扰动，展现独特效果与攻击VLA特征空间的实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [4] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: 视频扩散模型帧保真高但运动一致性差，MoGAN是一种无需奖励模型的后训练框架，通过DiT光流判别器和分布匹配正则化提升运动真实性。


<details>
  <summary>Details</summary>
Motivation: 标准去噪MSE缺乏时序监督，导致视频生成出现抖动、鬼影或不合理动态，限制运动连贯性、动态性和真实性。

Method: 基于3步蒸馏视频扩散模型（Wan2.1-T2V-1.3B），训练DiT-based光学流判别器区分真实与生成运动，并结合分布匹配正则化保持视觉保真度。

Result: VBench运动分数提升+7.3%（超50步教师）、+13.3%（超3步DMD）；VideoJAM-Bench +7.4%/+8.8%；人类研究偏好MoGAN（52% vs 38%、56% vs 29%）；美学与图像质量相当或更好。

Conclusion: MoGAN显著提升运动真实性，不牺牲视觉保真度或效率，提供快速高质量视频生成的实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [5] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出一种自提示、点监督框架，通过Refine-Requery-Reinforce循环，仅用稀疏点标注将SAM适配到遥感图像（RSI），无需全掩码监督，提升分割质量和域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAM在自然图像上泛化出色，但在遥感图像上因域偏移和密集标注稀缺而表现不佳，需要一种仅用稀疏点标注的适配方法。

Method: 采用Refine-Requery-Reinforce循环：从初始点生成粗伪掩码（Refine），用自建框提示改进（Requery），跨迭代对齐嵌入以减少确认偏差（Reinforce），实现自引导提示适配。

Result: 在WHU、HRSID和NWPU VHR-10三个RSI基准数据集上，超越预训练SAM和近期点监督分割方法。

Conclusion: 自提示和语义对齐为基础分割模型的遥感应用提供高效、可扩展的点级适配路径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [6] [Active Learning for GCN-based Action Recognition](https://arxiv.org/abs/2511.21625)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.

</details>


### [7] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow是一个统一的框架，通过因果反事实去混淆和双向时间条件流整合，针对长期动作质量评估（AQA），解决扩展时序动态和上下文混杂因素的挑战，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 长期AQA（如花样滑冰、艺术体操）需建模长时序动态，同时对上下文混杂因素鲁棒。现方法依赖昂贵标注或单向时序建模，易受虚假相关性和不稳定表示影响。

Method: CaFlow整合Causal Counterfactual Regularization (CCR)模块（自监督解耦因果与混杂特征，通过反事实干预强制因果鲁棒性）和BiT-Flow模块（双向动态建模与循环一致性约束，生成更平滑连贯表示）。

Result: 在多个长期AQA基准上实现最先进性能。

Conclusion: CaFlow有效提升长期AQA性能，代码开源。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [8] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: 大型多模态模型（LMMs）作为多模态评估评判者，其遵循多元细粒度标准的性能未充分探索。本文提出Multi-Crit基准，评估其多元标准遵守、灵活性和冲突识别能力。通过分析25个LMM，发现专有模型在开放式任务中遵守不一致，开源模型更弱，批评微调无法泛化。


<details>
  <summary>Details</summary>
Motivation: LMMs因指令遵循性和人类偏好一致性被用作多模态评判者，但其处理多样化、细粒度评估标准的可靠能力未被充分研究，需要基准评估其局限性。

Method: 开发Multi-Crit基准，覆盖开放式生成和可验证推理任务；通过严格数据 curation 收集多标准人类标注的挑战响应对；引入三项新指标（多元遵守、标准切换灵活性、偏好冲突识别）；对25个LMM进行全面评估，包括微调、缩放和模型边界分析。

Result: 1)专有模型难以持续遵守多元标准，尤其开放式评估；2)开源模型在标准灵活性上落后；3)整体判断微调提升视觉 grounding 但不泛化到标准级判断；额外分析揭示推理微调、缩放和开源/专有边界局限。

Conclusion: Multi-Crit开创性奠基可靠、可控多模态AI评估，推动未来多模态评判者发展。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [9] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 仅从相机轨迹（无需像素）感知视频内容？本文首次系统探究此问题，提出CamFormer对比学习框架，将轨迹映射到与自然语言对齐的嵌入空间。证明轨迹能揭示视频内容（如做什么或观察什么），并在跨模态对齐、分类、时序分析等任务上鲁棒表现。


<details>
  <summary>Details</summary>
Motivation: 探究看似不可能的问题：仅凭相机轨迹路径能否感知视频内容？相机轨迹作为轻量、鲁棒信号，可能揭示“如何移动”对应“做什么”（egocentric）或“观察什么”（exocentric）。

Method: 提出对比学习框架训练CamFormer专用编码器，将相机姿态轨迹投影到联合嵌入空间，与自然语言对齐。

Result: 相机轨迹是揭示视频内容的丰富信号；在多样下游任务（如跨模态对齐、分类、时序分析）中表现优异；对高保真多传感器及标准RGB-only姿态估计方法均鲁棒。

Conclusion: 确立相机轨迹为感知视频内容的轻量、鲁棒、多功能模态。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [10] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Canvas-to-Image是一个统一框架，将多种异构控制信号（如文本提示、主体参考、空间布局、姿势约束）编码成单一画布图像，实现扩散模型的高保真组合生成。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在生成高质量、多样化图像方面表现出色，但难以实现高保真组合和多模态控制，特别是同时指定多种控制时。

Method: 将多样控制信号编码为单一复合画布图像； curation 多任务数据集；提出Multi-Task Canvas Training策略，在统一范式下联合训练扩散模型理解并整合异构控制。

Result: 在身份保存和控制遵守度上显著优于SOTA方法，涵盖多人物组合、姿势控制组合、布局约束生成和多控制生成等挑战基准。

Conclusion: 实现跨多模态控制的推理，而非依赖任务特定启发式，并在推理时泛化良好。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出一种简单、轻量且广泛适用的方法，通过将神经元激活与辅助指标（如外部标签和模型置信度）相关联，隔离编码特定技能的神经元。该方法扩展了先前在分类任务上通过软提示训练识别“技能神经元”的工作，适用于多技能复杂场景，并在开放文本生成、自然语言推理和BigBench算术推理任务上验证，有效检测已知技能神经元并揭示先前未识别的算术捷径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在各种任务上表现出色，但内部机制仍不透明。本文旨在揭示编码特定技能的神经元，以提升模型的可解释性。

Method: 基于先前软提示训练在分类任务上识别技能神经元的工作，扩展至多技能复杂场景。通过将单个神经元激活与外部标签和模型自身置信度等辅助指标相关联，识别可解释的任务特定行为，无需手动令牌聚合。

Result: 在开放式文本生成、自然语言推理任务上实证验证，成功检测驱动已知技能的神经元，并在BigBench算术推理中揭示先前未识别的捷径。

Conclusion: 该方法简单、轻量且通用，能够揭示LLM中可解释的任务特定行为和隐藏捷径。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [12] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文研究在LLM预训练中融入多种元数据加速训练，发现细粒度文档质量指标等有效，并提出元数据追加和可学习元标记方法，通过探针分析揭示机制，提供整合指南。


<details>
  <summary>Details</summary>
Motivation: 先前仅URL元数据有效，本文探索更广元数据类型，特别是细粒度信号，以进一步加速LLM预训练。

Method: 调查多种元数据，前置细粒度质量指标；引入元数据追加（辅助预测任务）；使用带掩码损失的可学习元标记；通过探针分析潜在表示。

Result: 细粒度元数据加速预训练；追加和元标记恢复部分加速并诱导质量感知结构；元数据塑造学习表示。

Conclusion: 有效元数据具有细粒度特征，提供实际指南以整合元数据提升LLM预训练的效率和效果。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [13] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 大型语言模型生成的捷克诗歌与人类作品难以区分，本地使用者辨识准确率仅45.8%（随机水平），审美评价存在作者归属偏差：被认为AI生成的作品评分较低，尽管AI诗歌实际评分相等或更高。诗歌熟悉度无影响。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成诗歌研究多聚焦英语（训练数据主导语言），本研究考察形态复杂、低资源斯拉夫语捷克语中AI与人类诗歌的感知、辨识及审美判断。

Method: 捷克母语者参与实验，判断诗歌作者（AI或人类），并进行审美评分；使用逻辑回归模型分析喜好与作者归属准确性的关系；考察诗歌/文学背景的影响。

Result: 作者归属猜测准确率平均45.8%（随机水平）；存在强烈作者偏差：认为AI生成时评分较低，尽管AI诗歌平均评分相等或更高；越喜欢诗歌，越难准确归属作者；诗歌或文学背景对辨识准确率无影响。

Conclusion: AI能在形态复杂、低资源语言如捷克语中生成令人信服的诗歌；读者对作者归属的信念与审美评价相互关联。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [14] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLMs）在不同任务难度下的泛化能力，使用数千LLMs输出和项目反应理论（IRT）对六个数据集示例进行难度排名，发现训练易或难数据无法在全难度范围一致提升性能，强调训练与评估数据需覆盖难度范围。


<details>
  <summary>Details</summary>
Motivation: 现有研究对训练易数据或难数据是否更好，以及收益是否在易/难测试数据上的结论混杂，需要更客观、大规模、细粒度的分析来澄清LLMs跨难度的泛化。

Method: 在六个数据集上，使用数千不同LLMs的输出结合IRT（教育测试标准难度指标）对示例进行难度排名，不依赖人类主观判断，实现模型、数据集和细粒度难度组的系统评估。

Result: 跨难度泛化往往受限；训练易数据或难数据均不能在全难度范围内实现一致改进。

Conclusion: LLMs训练和评估数据必须包含难度范围，避免仅用易/难数据的捷径策略有风险。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）在基准测试中表现出色，但规划和有状态推理能力不明朗。本文使用8数独直接评估这些能力，结果显示LLMs在无外部工具时存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: LLMs在许多基准上取得令人印象深刻的结果，但其规划和有状态推理能力仍不清楚，需要直接评估。

Method: 使用8数独任务测试四个模型，采用零样本、思维链、思维算法提示，并提供分级纠正反馈和外部移动验证器，进行定性和定量分析。

Result: 反馈提升部分模型成功率，但运行冗长昂贵；使用移动验证器时，无模型能解谜；定性分析显示内部状态表示脆弱（无效移动）和启发式规划弱（循环、非最优行动）。

Conclusion: 当前LLMs在无外部工具如代码解释器时，规划能力有限；未来需引入显式状态维护和结构化搜索机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [16] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出将系统动力学（System Dynamics）和结构方程建模（Structural Equation Modeling）整合到一个共同数学框架中，用于从分布生成系统、开发方法并比较结果，以克服不同方法假设冲突，推动负责任AI/ML开发。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型虽解决难题但放大人类偏见，负责任AI需借鉴系统动力学的丰富因果模型，但不同方法根植于不同假设（Dana Meadows“不可避免的先验”）导致整合困难。

Method: 构建系统动力学与结构方程建模的统一数学框架，支持从分布生成系统、方法开发及结果比较。

Result: 提供可用于数据科学和AI/ML的框架，促进方法比较与认识论探讨。

Conclusion: 该框架有助于将系统动力学应用于AI/ML，提升数据科学中的因果建模与负责任开发。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [17] [Resilient Charging Infrastructure via Decentralized Coordination of Electric Vehicles at Scale](https://arxiv.org/abs/2511.20943)
*Chuhao Qin,Alexandru Sorici,Andrei Olaru,Evangelos Pournaras,Adina Magda Florea*

Main category: cs.MA

TL;DR: 提出基于集体学习的去中心化电动车充电协调框架，在充电站故障或请求激增等突发情况下，EV通过自适应行为平衡个体舒适性和系统整体队列效率，实现帕累托最优。实验验证优于基准，减少等待时间，并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化充电控制方法虽能协调EV选择充电站、降低成本、避免峰值并保护隐私，但在站故障或请求激增等严重突发下，竞争激烈导致长队和舒适度降低。

Method: 新型集体学习协调框架：推荐EV自适应充电行为，在个体舒适（选择）和系统效率（整体队列）间动态切换优先级，实现不同容量和时空分布下的帕累托最优权衡。

Result: 真实EV和充电站数据实验显示，优于基准方法，显著减少旅行和排队时间；不确定条件下，适时自私或利他行为比始终中等行为等待时间短；在高故障率和对抗性EV下，提升基础设施弹性和可信度。

Conclusion: 该方法增强去中心化EV充电系统的韧性和可信赖性，揭示动态行为调整在不确定环境中的优势。

Abstract: The rapid adoption of electric vehicles (EVs) introduces major challenges for decentralized charging control. Existing decentralized approaches efficiently coordinate a large number of EVs to select charging stations while reducing energy costs, preventing power peak and preserving driver privacy. However, they often struggle under severe contingencies, such as station outages or unexpected surges in charging requests. These situations create competition for limited charging slots, resulting in long queues and reduced driver comfort. To address these limitations, we propose a novel collective learning-based coordination framework that allows EVs to balance individual comfort on their selections against system-wide efficiency, i.e., the overall queues across all stations. In the framework, EVs are recommended for adaptive charging behaviors that shift priority between comfort and efficiency, achieving Pareto-optimal trade-offs under varying station capacities and dynamic spatio-temporal EV distribution. Experiments using real-world data from EVs and charging stations show that the proposed approach outperforms baseline methods, significantly reducing travel and queuing time. The results reveal that, under uncertain charging conditions, EV drivers that behave selfishly or altruistically at the right moments achieve shorter waiting time than those maintaining moderate behavior throughout. Our findings under high fractions of station outages and adversarial EVs further demonstrate improved resilience and trustworthiness of decentralized EV charging infrastructure.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models](https://arxiv.org/abs/2511.20732)
*Ziyuan Gao,Philippe Morel*

Main category: cs.MM

TL;DR: 提出PA-EWC，一种提示感知的自适应弹性权重整合方法，用于解决医疗视觉-语言模型在持续学习中的灾难性遗忘问题，通过参数分类和保护机制在新协议下适应，同时保留既有知识。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在临床环境中部署时面临灾难性遗忘，即学习新成像协议时丢失先前诊断能力，尤其是视觉-语言模型需维持图像与临床术语间的跨模态对齐，针对多样成像模态。

Method: 系统分类模型参数（视觉-描述、空间引导、医学术语语义），使用提示引导的参数专化；融入自适应Fisher信息计算与梯度稳定性分析；开发基于医学术语密度的加权复杂度指标，实现关键知识保护与新需求适应。

Result: 在Kvasir-SEG、ISIC 2018、CheXlocalize、BUSI、CAMUS五个数据集（内镜、皮肤镜、X光、超声）上评估，比基线减少灾难性遗忘高达17.58%，胸部X光病理定位提升4.30%，息肉分割提升6.06%。

Conclusion: PA-EWC显著缓解医疗视觉-语言模型的灾难性遗忘，提高跨模态任务性能，是临床持续学习的有力方法。

Abstract: Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.

</details>


### [19] [AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control](https://arxiv.org/abs/2511.21146)
*Xinyue Guo,Xiaoran Yang,Lipan Zhang,Jianxuan Yang,Zhao Wang,Jian Luan*

Main category: cs.MM

TL;DR: 提出AV-Edit框架，通过视觉、音频和文本语义实现视频中现有音频轨道的细粒度编辑，克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有声音效果编辑方法依赖低级信号处理或粗糙文本提示，灵活性和音频质量不足。

Method: 使用对比式音频-视觉掩码自编码器（CAV-MAE-Edit）进行多模态预训练，学习跨模态对齐表示；然后训练编辑多模态扩散Transformer（MM-DiT），通过相关性特征门控策略移除无关声音并生成一致音频；构建专用视频声音编辑数据集。

Result: 生成高质量、精确修改的音频，在声音效果编辑领域达到SOTA性能，在音频生成领域具有竞争力。

Conclusion: AV-Edit基于视觉内容实现精确高品质音频编辑，显著优于现有方法。

Abstract: Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.

</details>


### [20] [PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots](https://arxiv.org/abs/2511.21244)
*Ziheng Guo,Tianxiang Wei,Zeyu Li,Lianghao Zhang,Sisi Li,Jiawan Zhang*

Main category: cs.MM

TL;DR: 大规模散点图重叠绘制不可避免，现抽象方法在中低密度区丢失特征。本文提出视觉抽象方法，在任意抽象水平更好地保留特征，尤其中低密度区。三步：分区等密度区并均衡视觉密度；在区内分配像素给类别；像素重构数据分布。用户研究及评估显示优于现有方法，特别适于超高动态范围数据。


<details>
  <summary>Details</summary>
Motivation: 大规模散点图中重叠绘制（overdraw）不可避免，现有的散点图抽象方法在中低密度区域丢失重要特征，需要一种跨任意抽象水平、特别在中低密度区更好地保留特征的视觉抽象方法。

Method: 三紧密相连步骤：1）将散点图分区为等密度（iso-density）区域并均衡化视觉密度；2）在每个区域内为不同类别分配像素；3）基于分配像素重构数据分布。

Result: 用户研究、定量和定性评估显示，与先前方法相比，本方法更好地保留特征，在处理超高动态范围数据分布时展现特别优势。

Conclusion: 本文方法在特征保留上优于现有方法，尤其适用于超高动态范围数据分布的散点图可视化。

Abstract: Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.

</details>
