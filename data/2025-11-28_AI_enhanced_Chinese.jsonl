{"id": "2511.20732", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20732", "abs": "https://arxiv.org/abs/2511.20732", "authors": ["Ziyuan Gao", "Philippe Morel"], "title": "Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models", "comment": "Accepted by 32nd International Conference on MultiMedia Modeling (MMM 2026)", "summary": "Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.", "AI": {"tldr": "\u63d0\u51faPA-EWC\uff0c\u4e00\u79cd\u63d0\u793a\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5f39\u6027\u6743\u91cd\u6574\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u5206\u7c7b\u548c\u4fdd\u62a4\u673a\u5236\u5728\u65b0\u534f\u8bae\u4e0b\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u7559\u65e2\u6709\u77e5\u8bc6\u3002", "motivation": "\u533b\u7597AI\u7cfb\u7edf\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5373\u5b66\u4e60\u65b0\u6210\u50cf\u534f\u8bae\u65f6\u4e22\u5931\u5148\u524d\u8bca\u65ad\u80fd\u529b\uff0c\u5c24\u5176\u662f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9700\u7ef4\u6301\u56fe\u50cf\u4e0e\u4e34\u5e8a\u672f\u8bed\u95f4\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u9488\u5bf9\u591a\u6837\u6210\u50cf\u6a21\u6001\u3002", "method": "\u7cfb\u7edf\u5206\u7c7b\u6a21\u578b\u53c2\u6570\uff08\u89c6\u89c9-\u63cf\u8ff0\u3001\u7a7a\u95f4\u5f15\u5bfc\u3001\u533b\u5b66\u672f\u8bed\u8bed\u4e49\uff09\uff0c\u4f7f\u7528\u63d0\u793a\u5f15\u5bfc\u7684\u53c2\u6570\u4e13\u5316\uff1b\u878d\u5165\u81ea\u9002\u5e94Fisher\u4fe1\u606f\u8ba1\u7b97\u4e0e\u68af\u5ea6\u7a33\u5b9a\u6027\u5206\u6790\uff1b\u5f00\u53d1\u57fa\u4e8e\u533b\u5b66\u672f\u8bed\u5bc6\u5ea6\u7684\u52a0\u6743\u590d\u6742\u5ea6\u6307\u6807\uff0c\u5b9e\u73b0\u5173\u952e\u77e5\u8bc6\u4fdd\u62a4\u4e0e\u65b0\u9700\u6c42\u9002\u5e94\u3002", "result": "\u5728Kvasir-SEG\u3001ISIC 2018\u3001CheXlocalize\u3001BUSI\u3001CAMUS\u4e94\u4e2a\u6570\u636e\u96c6\uff08\u5185\u955c\u3001\u76ae\u80a4\u955c\u3001X\u5149\u3001\u8d85\u58f0\uff09\u4e0a\u8bc4\u4f30\uff0c\u6bd4\u57fa\u7ebf\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u9ad8\u8fbe17.58%\uff0c\u80f8\u90e8X\u5149\u75c5\u7406\u5b9a\u4f4d\u63d0\u53474.30%\uff0c\u606f\u8089\u5206\u5272\u63d0\u53476.06%\u3002", "conclusion": "PA-EWC\u663e\u8457\u7f13\u89e3\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u9ad8\u8de8\u6a21\u6001\u4efb\u52a1\u6027\u80fd\uff0c\u662f\u4e34\u5e8a\u6301\u7eed\u5b66\u4e60\u7684\u6709\u529b\u65b9\u6cd5\u3002"}}
{"id": "2511.21146", "categories": ["cs.MM", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21146", "abs": "https://arxiv.org/abs/2511.21146", "authors": ["Xinyue Guo", "Xiaoran Yang", "Lipan Zhang", "Jianxuan Yang", "Zhao Wang", "Jian Luan"], "title": "AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control", "comment": null, "summary": "Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.", "AI": {"tldr": "\u63d0\u51faAV-Edit\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u8bed\u4e49\u5b9e\u73b0\u89c6\u9891\u4e2d\u73b0\u6709\u97f3\u9891\u8f68\u9053\u7684\u7ec6\u7c92\u5ea6\u7f16\u8f91\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u58f0\u97f3\u6548\u679c\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u7ea7\u4fe1\u53f7\u5904\u7406\u6216\u7c97\u7cd9\u6587\u672c\u63d0\u793a\uff0c\u7075\u6d3b\u6027\u548c\u97f3\u9891\u8d28\u91cf\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5f0f\u97f3\u9891-\u89c6\u89c9\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08CAV-MAE-Edit\uff09\u8fdb\u884c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u8868\u793a\uff1b\u7136\u540e\u8bad\u7ec3\u7f16\u8f91\u591a\u6a21\u6001\u6269\u6563Transformer\uff08MM-DiT\uff09\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u7279\u5f81\u95e8\u63a7\u7b56\u7565\u79fb\u9664\u65e0\u5173\u58f0\u97f3\u5e76\u751f\u6210\u4e00\u81f4\u97f3\u9891\uff1b\u6784\u5efa\u4e13\u7528\u89c6\u9891\u58f0\u97f3\u7f16\u8f91\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7cbe\u786e\u4fee\u6539\u7684\u97f3\u9891\uff0c\u5728\u58f0\u97f3\u6548\u679c\u7f16\u8f91\u9886\u57df\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u97f3\u9891\u751f\u6210\u9886\u57df\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "AV-Edit\u57fa\u4e8e\u89c6\u89c9\u5185\u5bb9\u5b9e\u73b0\u7cbe\u786e\u9ad8\u54c1\u8d28\u97f3\u9891\u7f16\u8f91\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.21244", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.21244", "abs": "https://arxiv.org/abs/2511.21244", "authors": ["Ziheng Guo", "Tianxiang Wei", "Zeyu Li", "Lianghao Zhang", "Sisi Li", "Jiawan Zhang"], "title": "PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots", "comment": null, "summary": "Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.", "AI": {"tldr": "\u5927\u89c4\u6a21\u6563\u70b9\u56fe\u91cd\u53e0\u7ed8\u5236\u4e0d\u53ef\u907f\u514d\uff0c\u73b0\u62bd\u8c61\u65b9\u6cd5\u5728\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u4e22\u5931\u7279\u5f81\u3002\u672c\u6587\u63d0\u51fa\u89c6\u89c9\u62bd\u8c61\u65b9\u6cd5\uff0c\u5728\u4efb\u610f\u62bd\u8c61\u6c34\u5e73\u66f4\u597d\u5730\u4fdd\u7559\u7279\u5f81\uff0c\u5c24\u5176\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u3002\u4e09\u6b65\uff1a\u5206\u533a\u7b49\u5bc6\u5ea6\u533a\u5e76\u5747\u8861\u89c6\u89c9\u5bc6\u5ea6\uff1b\u5728\u533a\u5185\u5206\u914d\u50cf\u7d20\u7ed9\u7c7b\u522b\uff1b\u50cf\u7d20\u91cd\u6784\u6570\u636e\u5206\u5e03\u3002\u7528\u6237\u7814\u7a76\u53ca\u8bc4\u4f30\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u4e8e\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u3002", "motivation": "\u5927\u89c4\u6a21\u6563\u70b9\u56fe\u4e2d\u91cd\u53e0\u7ed8\u5236\uff08overdraw\uff09\u4e0d\u53ef\u907f\u514d\uff0c\u73b0\u6709\u7684\u6563\u70b9\u56fe\u62bd\u8c61\u65b9\u6cd5\u5728\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u57df\u4e22\u5931\u91cd\u8981\u7279\u5f81\uff0c\u9700\u8981\u4e00\u79cd\u8de8\u4efb\u610f\u62bd\u8c61\u6c34\u5e73\u3001\u7279\u522b\u5728\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u66f4\u597d\u5730\u4fdd\u7559\u7279\u5f81\u7684\u89c6\u89c9\u62bd\u8c61\u65b9\u6cd5\u3002", "method": "\u4e09\u7d27\u5bc6\u76f8\u8fde\u6b65\u9aa4\uff1a1\uff09\u5c06\u6563\u70b9\u56fe\u5206\u533a\u4e3a\u7b49\u5bc6\u5ea6\uff08iso-density\uff09\u533a\u57df\u5e76\u5747\u8861\u5316\u89c6\u89c9\u5bc6\u5ea6\uff1b2\uff09\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u4e3a\u4e0d\u540c\u7c7b\u522b\u5206\u914d\u50cf\u7d20\uff1b3\uff09\u57fa\u4e8e\u5206\u914d\u50cf\u7d20\u91cd\u6784\u6570\u636e\u5206\u5e03\u3002", "result": "\u7528\u6237\u7814\u7a76\u3001\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u66f4\u597d\u5730\u4fdd\u7559\u7279\u5f81\uff0c\u5728\u5904\u7406\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u5206\u5e03\u65f6\u5c55\u73b0\u7279\u522b\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u7279\u5f81\u4fdd\u7559\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u5206\u5e03\u7684\u6563\u70b9\u56fe\u53ef\u89c6\u5316\u3002"}}
{"id": "2511.20943", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20943", "abs": "https://arxiv.org/abs/2511.20943", "authors": ["Chuhao Qin", "Alexandru Sorici", "Andrei Olaru", "Evangelos Pournaras", "Adina Magda Florea"], "title": "Resilient Charging Infrastructure via Decentralized Coordination of Electric Vehicles at Scale", "comment": "14 pages, 12 figures. This work has been submitted to the IEEE for possible publication", "summary": "The rapid adoption of electric vehicles (EVs) introduces major challenges for decentralized charging control. Existing decentralized approaches efficiently coordinate a large number of EVs to select charging stations while reducing energy costs, preventing power peak and preserving driver privacy. However, they often struggle under severe contingencies, such as station outages or unexpected surges in charging requests. These situations create competition for limited charging slots, resulting in long queues and reduced driver comfort. To address these limitations, we propose a novel collective learning-based coordination framework that allows EVs to balance individual comfort on their selections against system-wide efficiency, i.e., the overall queues across all stations. In the framework, EVs are recommended for adaptive charging behaviors that shift priority between comfort and efficiency, achieving Pareto-optimal trade-offs under varying station capacities and dynamic spatio-temporal EV distribution. Experiments using real-world data from EVs and charging stations show that the proposed approach outperforms baseline methods, significantly reducing travel and queuing time. The results reveal that, under uncertain charging conditions, EV drivers that behave selfishly or altruistically at the right moments achieve shorter waiting time than those maintaining moderate behavior throughout. Our findings under high fractions of station outages and adversarial EVs further demonstrate improved resilience and trustworthiness of decentralized EV charging infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u4f53\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7535\u52a8\u8f66\u5145\u7535\u534f\u8c03\u6846\u67b6\uff0c\u5728\u5145\u7535\u7ad9\u6545\u969c\u6216\u8bf7\u6c42\u6fc0\u589e\u7b49\u7a81\u53d1\u60c5\u51b5\u4e0b\uff0cEV\u901a\u8fc7\u81ea\u9002\u5e94\u884c\u4e3a\u5e73\u8861\u4e2a\u4f53\u8212\u9002\u6027\u548c\u7cfb\u7edf\u6574\u4f53\u961f\u5217\u6548\u7387\uff0c\u5b9e\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4f18\u4e8e\u57fa\u51c6\uff0c\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4\uff0c\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u5145\u7535\u63a7\u5236\u65b9\u6cd5\u867d\u80fd\u534f\u8c03EV\u9009\u62e9\u5145\u7535\u7ad9\u3001\u964d\u4f4e\u6210\u672c\u3001\u907f\u514d\u5cf0\u503c\u5e76\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u5728\u7ad9\u6545\u969c\u6216\u8bf7\u6c42\u6fc0\u589e\u7b49\u4e25\u91cd\u7a81\u53d1\u4e0b\uff0c\u7ade\u4e89\u6fc0\u70c8\u5bfc\u81f4\u957f\u961f\u548c\u8212\u9002\u5ea6\u964d\u4f4e\u3002", "method": "\u65b0\u578b\u96c6\u4f53\u5b66\u4e60\u534f\u8c03\u6846\u67b6\uff1a\u63a8\u8350EV\u81ea\u9002\u5e94\u5145\u7535\u884c\u4e3a\uff0c\u5728\u4e2a\u4f53\u8212\u9002\uff08\u9009\u62e9\uff09\u548c\u7cfb\u7edf\u6548\u7387\uff08\u6574\u4f53\u961f\u5217\uff09\u95f4\u52a8\u6001\u5207\u6362\u4f18\u5148\u7ea7\uff0c\u5b9e\u73b0\u4e0d\u540c\u5bb9\u91cf\u548c\u65f6\u7a7a\u5206\u5e03\u4e0b\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "result": "\u771f\u5b9eEV\u548c\u5145\u7535\u7ad9\u6570\u636e\u5b9e\u9a8c\u663e\u793a\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u65c5\u884c\u548c\u6392\u961f\u65f6\u95f4\uff1b\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\uff0c\u9002\u65f6\u81ea\u79c1\u6216\u5229\u4ed6\u884c\u4e3a\u6bd4\u59cb\u7ec8\u4e2d\u7b49\u884c\u4e3a\u7b49\u5f85\u65f6\u95f4\u77ed\uff1b\u5728\u9ad8\u6545\u969c\u7387\u548c\u5bf9\u6297\u6027EV\u4e0b\uff0c\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u5f39\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u53bb\u4e2d\u5fc3\u5316EV\u5145\u7535\u7cfb\u7edf\u7684\u97e7\u6027\u548c\u53ef\u4fe1\u8d56\u6027\uff0c\u63ed\u793a\u52a8\u6001\u884c\u4e3a\u8c03\u6574\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.21591", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21591", "abs": "https://arxiv.org/abs/2511.21591", "authors": ["Charles Schepanowski", "Charles Ling"], "title": "On the Limits of Innate Planning in Large Language Models", "comment": "33 pages, 7 figures", "summary": "Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u89c4\u5212\u548c\u6709\u72b6\u6001\u63a8\u7406\u80fd\u529b\u4e0d\u660e\u6717\u3002\u672c\u6587\u4f7f\u75288\u6570\u72ec\u76f4\u63a5\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793aLLMs\u5728\u65e0\u5916\u90e8\u5de5\u5177\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "LLMs\u5728\u8bb8\u591a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5176\u89c4\u5212\u548c\u6709\u72b6\u6001\u63a8\u7406\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u76f4\u63a5\u8bc4\u4f30\u3002", "method": "\u4f7f\u75288\u6570\u72ec\u4efb\u52a1\u6d4b\u8bd5\u56db\u4e2a\u6a21\u578b\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u7b97\u6cd5\u63d0\u793a\uff0c\u5e76\u63d0\u4f9b\u5206\u7ea7\u7ea0\u6b63\u53cd\u9988\u548c\u5916\u90e8\u79fb\u52a8\u9a8c\u8bc1\u5668\uff0c\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u53cd\u9988\u63d0\u5347\u90e8\u5206\u6a21\u578b\u6210\u529f\u7387\uff0c\u4f46\u8fd0\u884c\u5197\u957f\u6602\u8d35\uff1b\u4f7f\u7528\u79fb\u52a8\u9a8c\u8bc1\u5668\u65f6\uff0c\u65e0\u6a21\u578b\u80fd\u89e3\u8c1c\uff1b\u5b9a\u6027\u5206\u6790\u663e\u793a\u5185\u90e8\u72b6\u6001\u8868\u793a\u8106\u5f31\uff08\u65e0\u6548\u79fb\u52a8\uff09\u548c\u542f\u53d1\u5f0f\u89c4\u5212\u5f31\uff08\u5faa\u73af\u3001\u975e\u6700\u4f18\u884c\u52a8\uff09\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u65e0\u5916\u90e8\u5de5\u5177\u5982\u4ee3\u7801\u89e3\u91ca\u5668\u65f6\uff0c\u89c4\u5212\u80fd\u529b\u6709\u9650\uff1b\u672a\u6765\u9700\u5f15\u5165\u663e\u5f0f\u72b6\u6001\u7ef4\u62a4\u548c\u7ed3\u6784\u5316\u641c\u7d22\u673a\u5236\u3002"}}
{"id": "2511.21636", "categories": ["cs.AI", "stat.AP", "stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21636", "abs": "https://arxiv.org/abs/2511.21636", "authors": ["Peter S. Hovmand", "Kari O'Donnell", "Callie Ogland-Hand", "Brian Biroscak", "Douglas D. Gunzler"], "title": "Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling", "comment": "Presented at 43rd Conference of the International System Dynamics Society in Boston, United States", "summary": "AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's \"the unavoidable a priori\"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\uff08System Dynamics\uff09\u548c\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\uff08Structural Equation Modeling\uff09\u6574\u5408\u5230\u4e00\u4e2a\u5171\u540c\u6570\u5b66\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u4ece\u5206\u5e03\u751f\u6210\u7cfb\u7edf\u3001\u5f00\u53d1\u65b9\u6cd5\u5e76\u6bd4\u8f83\u7ed3\u679c\uff0c\u4ee5\u514b\u670d\u4e0d\u540c\u65b9\u6cd5\u5047\u8bbe\u51b2\u7a81\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efbAI/ML\u5f00\u53d1\u3002", "motivation": "AI/ML\u6a21\u578b\u867d\u89e3\u51b3\u96be\u9898\u4f46\u653e\u5927\u4eba\u7c7b\u504f\u89c1\uff0c\u8d1f\u8d23\u4efbAI\u9700\u501f\u9274\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u4e30\u5bcc\u56e0\u679c\u6a21\u578b\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u6839\u690d\u4e8e\u4e0d\u540c\u5047\u8bbe\uff08Dana Meadows\u201c\u4e0d\u53ef\u907f\u514d\u7684\u5148\u9a8c\u201d\uff09\u5bfc\u81f4\u6574\u5408\u56f0\u96be\u3002", "method": "\u6784\u5efa\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0e\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u7684\u7edf\u4e00\u6570\u5b66\u6846\u67b6\uff0c\u652f\u6301\u4ece\u5206\u5e03\u751f\u6210\u7cfb\u7edf\u3001\u65b9\u6cd5\u5f00\u53d1\u53ca\u7ed3\u679c\u6bd4\u8f83\u3002", "result": "\u63d0\u4f9b\u53ef\u7528\u4e8e\u6570\u636e\u79d1\u5b66\u548cAI/ML\u7684\u6846\u67b6\uff0c\u4fc3\u8fdb\u65b9\u6cd5\u6bd4\u8f83\u4e0e\u8ba4\u8bc6\u8bba\u63a2\u8ba8\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u5e94\u7528\u4e8eAI/ML\uff0c\u63d0\u5347\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u56e0\u679c\u5efa\u6a21\u4e0e\u8d1f\u8d23\u4efb\u5f00\u53d1\u3002"}}
{"id": "2511.21610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21610", "abs": "https://arxiv.org/abs/2511.21610", "authors": ["Yixiu Zhao", "Xiaozhi Wang", "Zijun Yao", "Lei Hou", "Juanzi Li"], "title": "Auxiliary Metrics Help Decoding Skill Neurons in the Wild", "comment": "7 pages, 7 figures. Includes additional appendix", "summary": "Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified \"skill neurons\" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u3001\u8f7b\u91cf\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u8f85\u52a9\u6307\u6807\uff08\u5982\u5916\u90e8\u6807\u7b7e\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff09\u76f8\u5173\u8054\uff0c\u9694\u79bb\u7f16\u7801\u7279\u5b9a\u6280\u80fd\u7684\u795e\u7ecf\u5143\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5148\u524d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u901a\u8fc7\u8f6f\u63d0\u793a\u8bad\u7ec3\u8bc6\u522b\u201c\u6280\u80fd\u795e\u7ecf\u5143\u201d\u7684\u5de5\u4f5c\uff0c\u9002\u7528\u4e8e\u591a\u6280\u80fd\u590d\u6742\u573a\u666f\uff0c\u5e76\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548cBigBench\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u6709\u6548\u68c0\u6d4b\u5df2\u77e5\u6280\u80fd\u795e\u7ecf\u5143\u5e76\u63ed\u793a\u5148\u524d\u672a\u8bc6\u522b\u7684\u7b97\u672f\u6377\u5f84\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u900f\u660e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u7f16\u7801\u7279\u5b9a\u6280\u80fd\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u8f6f\u63d0\u793a\u8bad\u7ec3\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc6\u522b\u6280\u80fd\u795e\u7ecf\u5143\u7684\u5de5\u4f5c\uff0c\u6269\u5c55\u81f3\u591a\u6280\u80fd\u590d\u6742\u573a\u666f\u3002\u901a\u8fc7\u5c06\u5355\u4e2a\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u5916\u90e8\u6807\u7b7e\u548c\u6a21\u578b\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u7b49\u8f85\u52a9\u6307\u6807\u76f8\u5173\u8054\uff0c\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\uff0c\u65e0\u9700\u624b\u52a8\u4ee4\u724c\u805a\u5408\u3002", "result": "\u5728\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u6210\u529f\u68c0\u6d4b\u9a71\u52a8\u5df2\u77e5\u6280\u80fd\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u5728BigBench\u7b97\u672f\u63a8\u7406\u4e2d\u63ed\u793a\u5148\u524d\u672a\u8bc6\u522b\u7684\u6377\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u8f7b\u91cf\u4e14\u901a\u7528\uff0c\u80fd\u591f\u63ed\u793aLLM\u4e2d\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\u548c\u9690\u85cf\u6377\u5f84\u3002"}}
{"id": "2511.21613", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21613", "abs": "https://arxiv.org/abs/2511.21613", "authors": ["Dongyang Fan", "Diba Hashemi", "Sai Praneeth Karimireddy", "Martin Jaggi"], "title": "Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining", "comment": null, "summary": "Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u878d\u5165\u591a\u79cd\u5143\u6570\u636e\u52a0\u901f\u8bad\u7ec3\uff0c\u53d1\u73b0\u7ec6\u7c92\u5ea6\u6587\u6863\u8d28\u91cf\u6307\u6807\u7b49\u6709\u6548\uff0c\u5e76\u63d0\u51fa\u5143\u6570\u636e\u8ffd\u52a0\u548c\u53ef\u5b66\u4e60\u5143\u6807\u8bb0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u9488\u5206\u6790\u63ed\u793a\u673a\u5236\uff0c\u63d0\u4f9b\u6574\u5408\u6307\u5357\u3002", "motivation": "\u5148\u524d\u4ec5URL\u5143\u6570\u636e\u6709\u6548\uff0c\u672c\u6587\u63a2\u7d22\u66f4\u5e7f\u5143\u6570\u636e\u7c7b\u578b\uff0c\u7279\u522b\u662f\u7ec6\u7c92\u5ea6\u4fe1\u53f7\uff0c\u4ee5\u8fdb\u4e00\u6b65\u52a0\u901fLLM\u9884\u8bad\u7ec3\u3002", "method": "\u8c03\u67e5\u591a\u79cd\u5143\u6570\u636e\uff0c\u524d\u7f6e\u7ec6\u7c92\u5ea6\u8d28\u91cf\u6307\u6807\uff1b\u5f15\u5165\u5143\u6570\u636e\u8ffd\u52a0\uff08\u8f85\u52a9\u9884\u6d4b\u4efb\u52a1\uff09\uff1b\u4f7f\u7528\u5e26\u63a9\u7801\u635f\u5931\u7684\u53ef\u5b66\u4e60\u5143\u6807\u8bb0\uff1b\u901a\u8fc7\u63a2\u9488\u5206\u6790\u6f5c\u5728\u8868\u793a\u3002", "result": "\u7ec6\u7c92\u5ea6\u5143\u6570\u636e\u52a0\u901f\u9884\u8bad\u7ec3\uff1b\u8ffd\u52a0\u548c\u5143\u6807\u8bb0\u6062\u590d\u90e8\u5206\u52a0\u901f\u5e76\u8bf1\u5bfc\u8d28\u91cf\u611f\u77e5\u7ed3\u6784\uff1b\u5143\u6570\u636e\u5851\u9020\u5b66\u4e60\u8868\u793a\u3002", "conclusion": "\u6709\u6548\u5143\u6570\u636e\u5177\u6709\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u63d0\u4f9b\u5b9e\u9645\u6307\u5357\u4ee5\u6574\u5408\u5143\u6570\u636e\u63d0\u5347LLM\u9884\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2511.21629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21629", "abs": "https://arxiv.org/abs/2511.21629", "authors": ["Anna Marklov\u00e1", "Ond\u0159ej Vin\u0161", "Martina Vok\u00e1\u010dov\u00e1", "Ji\u0159\u00ed Mili\u010dka"], "title": "The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry", "comment": null, "summary": "Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6377\u514b\u8bd7\u6b4c\u4e0e\u4eba\u7c7b\u4f5c\u54c1\u96be\u4ee5\u533a\u5206\uff0c\u672c\u5730\u4f7f\u7528\u8005\u8fa8\u8bc6\u51c6\u786e\u7387\u4ec545.8%\uff08\u968f\u673a\u6c34\u5e73\uff09\uff0c\u5ba1\u7f8e\u8bc4\u4ef7\u5b58\u5728\u4f5c\u8005\u5f52\u5c5e\u504f\u5dee\uff1a\u88ab\u8ba4\u4e3aAI\u751f\u6210\u7684\u4f5c\u54c1\u8bc4\u5206\u8f83\u4f4e\uff0c\u5c3d\u7ba1AI\u8bd7\u6b4c\u5b9e\u9645\u8bc4\u5206\u76f8\u7b49\u6216\u66f4\u9ad8\u3002\u8bd7\u6b4c\u719f\u6089\u5ea6\u65e0\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u8bd7\u6b4c\u7814\u7a76\u591a\u805a\u7126\u82f1\u8bed\uff08\u8bad\u7ec3\u6570\u636e\u4e3b\u5bfc\u8bed\u8a00\uff09\uff0c\u672c\u7814\u7a76\u8003\u5bdf\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u65af\u62c9\u592b\u8bed\u6377\u514b\u8bed\u4e2dAI\u4e0e\u4eba\u7c7b\u8bd7\u6b4c\u7684\u611f\u77e5\u3001\u8fa8\u8bc6\u53ca\u5ba1\u7f8e\u5224\u65ad\u3002", "method": "\u6377\u514b\u6bcd\u8bed\u8005\u53c2\u4e0e\u5b9e\u9a8c\uff0c\u5224\u65ad\u8bd7\u6b4c\u4f5c\u8005\uff08AI\u6216\u4eba\u7c7b\uff09\uff0c\u5e76\u8fdb\u884c\u5ba1\u7f8e\u8bc4\u5206\uff1b\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5206\u6790\u559c\u597d\u4e0e\u4f5c\u8005\u5f52\u5c5e\u51c6\u786e\u6027\u7684\u5173\u7cfb\uff1b\u8003\u5bdf\u8bd7\u6b4c/\u6587\u5b66\u80cc\u666f\u7684\u5f71\u54cd\u3002", "result": "\u4f5c\u8005\u5f52\u5c5e\u731c\u6d4b\u51c6\u786e\u7387\u5e73\u574745.8%\uff08\u968f\u673a\u6c34\u5e73\uff09\uff1b\u5b58\u5728\u5f3a\u70c8\u4f5c\u8005\u504f\u5dee\uff1a\u8ba4\u4e3aAI\u751f\u6210\u65f6\u8bc4\u5206\u8f83\u4f4e\uff0c\u5c3d\u7ba1AI\u8bd7\u6b4c\u5e73\u5747\u8bc4\u5206\u76f8\u7b49\u6216\u66f4\u9ad8\uff1b\u8d8a\u559c\u6b22\u8bd7\u6b4c\uff0c\u8d8a\u96be\u51c6\u786e\u5f52\u5c5e\u4f5c\u8005\uff1b\u8bd7\u6b4c\u6216\u6587\u5b66\u80cc\u666f\u5bf9\u8fa8\u8bc6\u51c6\u786e\u7387\u65e0\u5f71\u54cd\u3002", "conclusion": "AI\u80fd\u5728\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u6377\u514b\u8bed\u4e2d\u751f\u6210\u4ee4\u4eba\u4fe1\u670d\u7684\u8bd7\u6b4c\uff1b\u8bfb\u8005\u5bf9\u4f5c\u8005\u5f52\u5c5e\u7684\u4fe1\u5ff5\u4e0e\u5ba1\u7f8e\u8bc4\u4ef7\u76f8\u4e92\u5173\u8054\u3002"}}
{"id": "2511.21692", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21692", "abs": "https://arxiv.org/abs/2511.21692", "authors": ["Yeganeh Kordi", "Nihal V. Nayak", "Max Zuo", "Ilana Nguyen", "Stephen H. Bach"], "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy", "comment": null, "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u7528\u6570\u5343LLMs\u8f93\u51fa\u548c\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u5bf9\u516d\u4e2a\u6570\u636e\u96c6\u793a\u4f8b\u8fdb\u884c\u96be\u5ea6\u6392\u540d\uff0c\u53d1\u73b0\u8bad\u7ec3\u6613\u6216\u96be\u6570\u636e\u65e0\u6cd5\u5728\u5168\u96be\u5ea6\u8303\u56f4\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u5f3a\u8c03\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u6570\u636e\u9700\u8986\u76d6\u96be\u5ea6\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u8bad\u7ec3\u6613\u6570\u636e\u6216\u96be\u6570\u636e\u662f\u5426\u66f4\u597d\uff0c\u4ee5\u53ca\u6536\u76ca\u662f\u5426\u5728\u6613/\u96be\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u7ed3\u8bba\u6df7\u6742\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u6765\u6f84\u6e05LLMs\u8de8\u96be\u5ea6\u7684\u6cdb\u5316\u3002", "method": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u6570\u5343\u4e0d\u540cLLMs\u7684\u8f93\u51fa\u7ed3\u5408IRT\uff08\u6559\u80b2\u6d4b\u8bd5\u6807\u51c6\u96be\u5ea6\u6307\u6807\uff09\u5bf9\u793a\u4f8b\u8fdb\u884c\u96be\u5ea6\u6392\u540d\uff0c\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u4e3b\u89c2\u5224\u65ad\uff0c\u5b9e\u73b0\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u7ec6\u7c92\u5ea6\u96be\u5ea6\u7ec4\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u8de8\u96be\u5ea6\u6cdb\u5316\u5f80\u5f80\u53d7\u9650\uff1b\u8bad\u7ec3\u6613\u6570\u636e\u6216\u96be\u6570\u636e\u5747\u4e0d\u80fd\u5728\u5168\u96be\u5ea6\u8303\u56f4\u5185\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "LLMs\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u5fc5\u987b\u5305\u542b\u96be\u5ea6\u8303\u56f4\uff0c\u907f\u514d\u4ec5\u7528\u6613/\u96be\u6570\u636e\u7684\u6377\u5f84\u7b56\u7565\u6709\u98ce\u9669\u3002"}}
{"id": "2511.21688", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21688", "abs": "https://arxiv.org/abs/2511.21688", "authors": ["Wenbo Hu", "Jingli Lin", "Yilin Long", "Yunlong Ran", "Lihan Jiang", "Yifan Wang", "Chenming Zhu", "Runsen Xu", "Tai Wang", "Jiangmiao Pang"], "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning", "comment": "code are released at https://github.com/InternRobotics/G2VLM", "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.", "AI": {"tldr": "VLMs\u7f3a\u4e4f\u7a7a\u95f4\u667a\u80fd\u9c81\u68d2\u6027\uff0c\u63d0\u51faG\u00b2VLM\uff0c\u901a\u8fc7\u5b66\u4e603D\u89c6\u89c9\u51e0\u4f55\u7279\u5f81\u6865\u63a53D\u91cd\u5efa\u4e0e\u7a7a\u95f4\u7406\u89e3\uff0c\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u3002", "motivation": "VLMs\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\uff0c\u5f52\u56e0\u4e8e\u7f3a\u5c11\u4ece2D\u56fe\u50cf\u91cd\u5efa3D\u7a7a\u95f4\u7684\u89c6\u89c9\u51e0\u4f55\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "G\u00b2VLM\u662f\u4e00\u4e2a\u51e0\u4f55 grounding \u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u5b66\u4e60\u5230\u76843D\u89c6\u89c9\u51e0\u4f55\u7279\u5f81\u76f4\u63a5\u9884\u6d4b3D\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7in-context learning\u548c\u4ea4\u9519\u63a8\u7406\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\uff1b\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u540c\u65f6\u5229\u7528\u65e0\u9700\u786c\u6536\u96c6\u6807\u6ce8\u76843D\u89c6\u89c9\u5148\u9a8c\u3002", "result": "\u57283D\u91cd\u5efa\u4efb\u52a1\u4e0a\u4e0eSOTA\u524d\u9988\u6a21\u578b\u76f8\u5f53\uff0c\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u66f4\u597d\u6216\u7ade\u4e89\u529b\u3002", "conclusion": "\u7edf\u4e00\u8bed\u4e49\u5f3a\u7684VLM\u4e0e\u4f4e\u7ea73D\u89c6\u89c9\u4efb\u52a1\uff0c\u5e0c\u671b\u4f5c\u4e3a\u793e\u533a\u5f3a\u57fa\u7ebf\uff0c\u5f00\u542f3D\u573a\u666f\u7f16\u8f91\u7b49\u672a\u6765\u5e94\u7528\u3002"}}
{"id": "2511.21652", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21652", "abs": "https://arxiv.org/abs/2511.21652", "authors": ["Kirill Paramonov", "Mete Ozay", "Aristeidis Mystakidis", "Nikolaos Tsalikidis", "Dimitrios Sotos", "Anastasios Drosou", "Dimitrios Tzovaras", "Hyunjun Kim", "Kiseok Chang", "Sangdok Mo", "Namwoong Kim", "Woojong Yoo", "Jijoong Moon", "Umberto Michieli"], "title": "Continual Error Correction on Low-Resource Devices", "comment": "ACM MMSys 2025", "summary": "The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u7528\u6237\u5bf9AI\u8bef\u5206\u7c7b\u7684\u4f4e\u8d44\u6e90\u7ea0\u6b63\uff0c\u7ed3\u5408\u670d\u52a1\u5668\u7aef\u77e5\u8bc6\u84b8\u998f\u548c\u8bbe\u5907\u7aef\u539f\u578b\u5206\u7c7b\uff0c\u4ec5\u9700\u539f\u578b\u66f4\u65b0\u800c\u975e\u6a21\u578b\u91cd\u8bad\u3002", "motivation": "\u65e5\u5e38\u8bbe\u5907AI\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u635f\u5bb3\u7528\u6237\u4f53\u9a8c\uff0c\u73b0\u65b9\u6848\u591a\u9650\u4e8e\u9519\u8bef\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u9ad8\u6548\u7ea0\u6b63\u673a\u5236\u3002", "method": "\u670d\u52a1\u5668\u7aef\uff1a\u77e5\u8bc6\u84b8\u998f\u5c06\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8f6c\u79fb\u81f3\u8bbe\u5907\u67b6\u6784\uff1b\u8bbe\u5907\u7aef\uff1a\u57fa\u4e8e\u539f\u578b\u7684\u5206\u7c7b\uff0c\u901a\u8fc7\u539f\u578b\u9002\u5e94\u9ad8\u6548\u7ea0\u9519\u3002\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002", "result": "Food-101\u548cFlowers-102\u6570\u636e\u96c6\u4e0a\u4e00-shot\u7ea0\u9519\u7387\u8d8550%\uff0c\u9057\u5fd8\u7387<0.02%\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff1bAndroid app\u5b9e\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "\u7cfb\u7edf\u9ad8\u6548\u5b9e\u7528\uff0c\u663e\u8457\u63d0\u5347\u8bbe\u5907\u7aefAI\u7ea0\u9519\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2511.21663", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21663", "abs": "https://arxiv.org/abs/2511.21663", "authors": ["Naifu Zhang", "Wei Tao", "Xi Xiao", "Qianpu Sun", "Yuxin Zheng", "Wentao Mo", "Peiqiang Wang", "Nan Zhang"], "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models", "comment": null, "summary": "In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.", "AI": {"tldr": "\u63d0\u51faADVLA\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u89c6\u89c9\u7f16\u7801\u5668\u6295\u5f71\u5230\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u7684\u7279\u5f81\u4e0a\u65bd\u52a0\u4f4e\u5e45\u5ea6\u5bf9\u6297\u6270\u52a8\uff0c\u9ad8\u6548\u7834\u574fVLA\u6a21\u578b\u4e0b\u6e38\u52a8\u4f5c\u9884\u6d4b\uff0c\u6270\u52a8\u4e13\u6ce8\u7a00\u758f\uff0c\u51e0\u4e4e\u4e0d\u53ef\u5bdf\u89c9\u3002", "motivation": "\u73b0\u6709VLA\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u9700\u6602\u8d35\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4e14\u4ea7\u751f\u660e\u663e\u6270\u52a8\u8865\u4e01\uff0c\u9650\u5236\u5b9e\u7528\u6027\u3002", "method": "ADVLA\u6846\u67b6\u5728\u89c6\u89c9\u7f16\u7801\u5668\u6295\u5f71\u7279\u5f81\u4e0a\u76f4\u63a5\u65bd\u52a0\u6270\u52a8\u81f3\u6587\u672c\u7a7a\u95f4\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6307\u5bfc\u5b9e\u73b0\u6270\u52a8\u4e13\u6ce8\u4e0e\u7a00\u758f\uff1b\u5f15\u5165\u4e09\u79cd\u7b56\u7565\uff1a\u63d0\u5347\u654f\u611f\u6027\u3001\u5f3a\u5236\u7a00\u758f\u3001\u96c6\u4e2d\u6270\u52a8\u3002", "result": "L\u221e=4/255\u7ea6\u675f\u4e0b\uff0c\u7ed3\u5408Top-K\u63a9\u7801\u4fee\u6539&lt;10%\u8865\u4e01\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fd1100%\uff1b\u6270\u52a8\u96c6\u4e2d\u5173\u952e\u533a\u57df\uff0c\u51e0\u4e4e\u4e0d\u53ef\u5bdf\u89c9\uff1b\u5355\u6b65\u8fed\u4ee3\u4ec50.06\u79d2\uff0c\u8fdc\u8d85\u4f20\u7edf\u8865\u4e01\u653b\u51fb\u3002", "conclusion": "ADVLA\u5728\u4f4e\u5e45\u5ea6\u3001\u5c40\u90e8\u7a00\u758f\u6761\u4ef6\u4e0b\u6709\u6548\u524a\u5f31VLA\u52a8\u4f5c\u9884\u6d4b\uff0c\u907f\u514d\u9ad8\u8bad\u7ec3\u6210\u672c\u4e0e\u660e\u663e\u6270\u52a8\uff0c\u5c55\u73b0\u72ec\u7279\u6548\u679c\u4e0e\u653b\u51fbVLA\u7279\u5f81\u7a7a\u95f4\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.21592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21592", "abs": "https://arxiv.org/abs/2511.21592", "authors": ["Haotian Xue", "Qi Chen", "Zhonghao Wang", "Xun Huang", "Eli Shechtman", "Jinrong Xie", "Yongxin Chen"], "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training", "comment": null, "summary": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.", "code_url": "https://xavihart.github.io/mogan", "AI": {"tldr": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u5e27\u4fdd\u771f\u9ad8\u4f46\u8fd0\u52a8\u4e00\u81f4\u6027\u5dee\uff0cMoGAN\u662f\u4e00\u79cd\u65e0\u9700\u5956\u52b1\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7DiT\u5149\u6d41\u5224\u522b\u5668\u548c\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\u63d0\u5347\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "motivation": "\u6807\u51c6\u53bb\u566aMSE\u7f3a\u4e4f\u65f6\u5e8f\u76d1\u7763\uff0c\u5bfc\u81f4\u89c6\u9891\u751f\u6210\u51fa\u73b0\u6296\u52a8\u3001\u9b3c\u5f71\u6216\u4e0d\u5408\u7406\u52a8\u6001\uff0c\u9650\u5236\u8fd0\u52a8\u8fde\u8d2f\u6027\u3001\u52a8\u6001\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u57fa\u4e8e3\u6b65\u84b8\u998f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08Wan2.1-T2V-1.3B\uff09\uff0c\u8bad\u7ec3DiT-based\u5149\u5b66\u6d41\u5224\u522b\u5668\u533a\u5206\u771f\u5b9e\u4e0e\u751f\u6210\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "VBench\u8fd0\u52a8\u5206\u6570\u63d0\u5347+7.3%\uff08\u8d8550\u6b65\u6559\u5e08\uff09\u3001+13.3%\uff08\u8d853\u6b65DMD\uff09\uff1bVideoJAM-Bench +7.4%/+8.8%\uff1b\u4eba\u7c7b\u7814\u7a76\u504f\u597dMoGAN\uff0852% vs 38%\u300156% vs 29%\uff09\uff1b\u7f8e\u5b66\u4e0e\u56fe\u50cf\u8d28\u91cf\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "MoGAN\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u771f\u5b9e\u6027\uff0c\u4e0d\u727a\u7272\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u6548\u7387\uff0c\u63d0\u4f9b\u5feb\u901f\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.21606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21606", "abs": "https://arxiv.org/abs/2511.21606", "authors": ["M. Naseer Subhani"], "title": "ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images", "comment": null, "summary": "Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u63d0\u793a\u3001\u70b9\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7Refine-Requery-Reinforce\u5faa\u73af\uff0c\u4ec5\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u5c06SAM\u9002\u914d\u5230\u9065\u611f\u56fe\u50cf\uff08RSI\uff09\uff0c\u65e0\u9700\u5168\u63a9\u7801\u76d1\u7763\uff0c\u63d0\u5347\u5206\u5272\u8d28\u91cf\u548c\u57df\u9c81\u68d2\u6027\u3002", "motivation": "SAM\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u6cdb\u5316\u51fa\u8272\uff0c\u4f46\u5728\u9065\u611f\u56fe\u50cf\u4e0a\u56e0\u57df\u504f\u79fb\u548c\u5bc6\u96c6\u6807\u6ce8\u7a00\u7f3a\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u7684\u9002\u914d\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Refine-Requery-Reinforce\u5faa\u73af\uff1a\u4ece\u521d\u59cb\u70b9\u751f\u6210\u7c97\u4f2a\u63a9\u7801\uff08Refine\uff09\uff0c\u7528\u81ea\u5efa\u6846\u63d0\u793a\u6539\u8fdb\uff08Requery\uff09\uff0c\u8de8\u8fed\u4ee3\u5bf9\u9f50\u5d4c\u5165\u4ee5\u51cf\u5c11\u786e\u8ba4\u504f\u5dee\uff08Reinforce\uff09\uff0c\u5b9e\u73b0\u81ea\u5f15\u5bfc\u63d0\u793a\u9002\u914d\u3002", "result": "\u5728WHU\u3001HRSID\u548cNWPU VHR-10\u4e09\u4e2aRSI\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8d85\u8d8a\u9884\u8bad\u7ec3SAM\u548c\u8fd1\u671f\u70b9\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u63d0\u793a\u548c\u8bed\u4e49\u5bf9\u9f50\u4e3a\u57fa\u7840\u5206\u5272\u6a21\u578b\u7684\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u70b9\u7ea7\u9002\u914d\u8def\u5f84\u3002"}}
{"id": "2511.21625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21625", "abs": "https://arxiv.org/abs/2511.21625", "authors": ["Hichem Sahbi"], "title": "Active Learning for GCN-based Action Recognition", "comment": null, "summary": "Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21653", "abs": "https://arxiv.org/abs/2511.21653", "authors": ["Ruisheng Han", "Kanglei Zhou", "Shuang Chen", "Amir Atapour-Abarghouei", "Hubert P. H. Shum"], "title": "CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow", "comment": null, "summary": "Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow", "code_url": "https://github.com/Harrison21/CaFlow", "code_stars": 0, "code_last_update": "2025-11-27", "AI": {"tldr": "CaFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u53cd\u4e8b\u5b9e\u53bb\u6df7\u6dc6\u548c\u53cc\u5411\u65f6\u95f4\u6761\u4ef6\u6d41\u6574\u5408\uff0c\u9488\u5bf9\u957f\u671f\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\uff08AQA\uff09\uff0c\u89e3\u51b3\u6269\u5c55\u65f6\u5e8f\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u6df7\u6742\u56e0\u7d20\u7684\u6311\u6218\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u957f\u671fAQA\uff08\u5982\u82b1\u6837\u6ed1\u51b0\u3001\u827a\u672f\u4f53\u64cd\uff09\u9700\u5efa\u6a21\u957f\u65f6\u5e8f\u52a8\u6001\uff0c\u540c\u65f6\u5bf9\u4e0a\u4e0b\u6587\u6df7\u6742\u56e0\u7d20\u9c81\u68d2\u3002\u73b0\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u6216\u5355\u5411\u65f6\u5e8f\u5efa\u6a21\uff0c\u6613\u53d7\u865a\u5047\u76f8\u5173\u6027\u548c\u4e0d\u7a33\u5b9a\u8868\u793a\u5f71\u54cd\u3002", "method": "CaFlow\u6574\u5408Causal Counterfactual Regularization (CCR)\u6a21\u5757\uff08\u81ea\u76d1\u7763\u89e3\u8026\u56e0\u679c\u4e0e\u6df7\u6742\u7279\u5f81\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5e72\u9884\u5f3a\u5236\u56e0\u679c\u9c81\u68d2\u6027\uff09\u548cBiT-Flow\u6a21\u5757\uff08\u53cc\u5411\u52a8\u6001\u5efa\u6a21\u4e0e\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u751f\u6210\u66f4\u5e73\u6ed1\u8fde\u8d2f\u8868\u793a\uff09\u3002", "result": "\u5728\u591a\u4e2a\u957f\u671fAQA\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CaFlow\u6709\u6548\u63d0\u5347\u957f\u671fAQA\u6027\u80fd\uff0c\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2511.21662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21662", "abs": "https://arxiv.org/abs/2511.21662", "authors": ["Tianyi Xiong", "Yi Ge", "Ming Li", "Zuolong Zhang", "Pranav Kulkarni", "Kaishen Wang", "Qi He", "Zeying Zhu", "Chenxi Liu", "Ruibo Chen", "Tong Zheng", "Yanshuo Chen", "Xiyao Wang", "Renrui Zhang", "Wenhu Chen", "Heng Huang"], "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "comment": null, "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "AI": {"tldr": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4f5c\u4e3a\u591a\u6a21\u6001\u8bc4\u4f30\u8bc4\u5224\u8005\uff0c\u5176\u9075\u5faa\u591a\u5143\u7ec6\u7c92\u5ea6\u6807\u51c6\u7684\u6027\u80fd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u63d0\u51faMulti-Crit\u57fa\u51c6\uff0c\u8bc4\u4f30\u5176\u591a\u5143\u6807\u51c6\u9075\u5b88\u3001\u7075\u6d3b\u6027\u548c\u51b2\u7a81\u8bc6\u522b\u80fd\u529b\u3002\u901a\u8fc7\u5206\u679025\u4e2aLMM\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u9075\u5b88\u4e0d\u4e00\u81f4\uff0c\u5f00\u6e90\u6a21\u578b\u66f4\u5f31\uff0c\u6279\u8bc4\u5fae\u8c03\u65e0\u6cd5\u6cdb\u5316\u3002", "motivation": "LMMs\u56e0\u6307\u4ee4\u9075\u5faa\u6027\u548c\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u88ab\u7528\u4f5c\u591a\u6a21\u6001\u8bc4\u5224\u8005\uff0c\u4f46\u5176\u5904\u7406\u591a\u6837\u5316\u3001\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6807\u51c6\u7684\u53ef\u9760\u80fd\u529b\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u57fa\u51c6\u8bc4\u4f30\u5176\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1Multi-Crit\u57fa\u51c6\uff0c\u8986\u76d6\u5f00\u653e\u5f0f\u751f\u6210\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\uff1b\u901a\u8fc7\u4e25\u683c\u6570\u636e curation \u6536\u96c6\u591a\u6807\u51c6\u4eba\u7c7b\u6807\u6ce8\u7684\u6311\u6218\u54cd\u5e94\u5bf9\uff1b\u5f15\u5165\u4e09\u9879\u65b0\u6307\u6807\uff08\u591a\u5143\u9075\u5b88\u3001\u6807\u51c6\u5207\u6362\u7075\u6d3b\u6027\u3001\u504f\u597d\u51b2\u7a81\u8bc6\u522b\uff09\uff1b\u5bf925\u4e2aLMM\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5305\u62ec\u5fae\u8c03\u3001\u7f29\u653e\u548c\u6a21\u578b\u8fb9\u754c\u5206\u6790\u3002", "result": "1)\u4e13\u6709\u6a21\u578b\u96be\u4ee5\u6301\u7eed\u9075\u5b88\u591a\u5143\u6807\u51c6\uff0c\u5c24\u5176\u5f00\u653e\u5f0f\u8bc4\u4f30\uff1b2)\u5f00\u6e90\u6a21\u578b\u5728\u6807\u51c6\u7075\u6d3b\u6027\u4e0a\u843d\u540e\uff1b3)\u6574\u4f53\u5224\u65ad\u5fae\u8c03\u63d0\u5347\u89c6\u89c9 grounding \u4f46\u4e0d\u6cdb\u5316\u5230\u6807\u51c6\u7ea7\u5224\u65ad\uff1b\u989d\u5916\u5206\u6790\u63ed\u793a\u63a8\u7406\u5fae\u8c03\u3001\u7f29\u653e\u548c\u5f00\u6e90/\u4e13\u6709\u8fb9\u754c\u5c40\u9650\u3002", "conclusion": "Multi-Crit\u5f00\u521b\u6027\u5960\u57fa\u53ef\u9760\u3001\u53ef\u63a7\u591a\u6a21\u6001AI\u8bc4\u4f30\uff0c\u63a8\u52a8\u672a\u6765\u591a\u6a21\u6001\u8bc4\u5224\u8005\u53d1\u5c55\u3002"}}
{"id": "2511.21681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21681", "abs": "https://arxiv.org/abs/2511.21681", "authors": ["Zihui Xue", "Kristen Grauman", "Dima Damen", "Andrew Zisserman", "Tengda Han"], "title": "Seeing without Pixels: Perception from Camera Trajectories", "comment": "Project website: https://sites.google.com/view/seeing-without-pixels", "summary": "Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, \"how you move\" can indeed reveal \"what you are doing\" (egocentric) or \"observing\" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.", "AI": {"tldr": "\u4ec5\u4ece\u76f8\u673a\u8f68\u8ff9\uff08\u65e0\u9700\u50cf\u7d20\uff09\u611f\u77e5\u89c6\u9891\u5185\u5bb9\uff1f\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u63a2\u7a76\u6b64\u95ee\u9898\uff0c\u63d0\u51faCamFormer\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u6620\u5c04\u5230\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u5d4c\u5165\u7a7a\u95f4\u3002\u8bc1\u660e\u8f68\u8ff9\u80fd\u63ed\u793a\u89c6\u9891\u5185\u5bb9\uff08\u5982\u505a\u4ec0\u4e48\u6216\u89c2\u5bdf\u4ec0\u4e48\uff09\uff0c\u5e76\u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u5206\u7c7b\u3001\u65f6\u5e8f\u5206\u6790\u7b49\u4efb\u52a1\u4e0a\u9c81\u68d2\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u770b\u4f3c\u4e0d\u53ef\u80fd\u7684\u95ee\u9898\uff1a\u4ec5\u51ed\u76f8\u673a\u8f68\u8ff9\u8def\u5f84\u80fd\u5426\u611f\u77e5\u89c6\u9891\u5185\u5bb9\uff1f\u76f8\u673a\u8f68\u8ff9\u4f5c\u4e3a\u8f7b\u91cf\u3001\u9c81\u68d2\u4fe1\u53f7\uff0c\u53ef\u80fd\u63ed\u793a\u201c\u5982\u4f55\u79fb\u52a8\u201d\u5bf9\u5e94\u201c\u505a\u4ec0\u4e48\u201d\uff08egocentric\uff09\u6216\u201c\u89c2\u5bdf\u4ec0\u4e48\u201d\uff08exocentric\uff09\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3CamFormer\u4e13\u7528\u7f16\u7801\u5668\uff0c\u5c06\u76f8\u673a\u59ff\u6001\u8f68\u8ff9\u6295\u5f71\u5230\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u76f8\u673a\u8f68\u8ff9\u662f\u63ed\u793a\u89c6\u9891\u5185\u5bb9\u7684\u4e30\u5bcc\u4fe1\u53f7\uff1b\u5728\u591a\u6837\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u5206\u7c7b\u3001\u65f6\u5e8f\u5206\u6790\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff1b\u5bf9\u9ad8\u4fdd\u771f\u591a\u4f20\u611f\u5668\u53ca\u6807\u51c6RGB-only\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5747\u9c81\u68d2\u3002", "conclusion": "\u786e\u7acb\u76f8\u673a\u8f68\u8ff9\u4e3a\u611f\u77e5\u89c6\u9891\u5185\u5bb9\u7684\u8f7b\u91cf\u3001\u9c81\u68d2\u3001\u591a\u529f\u80fd\u6a21\u6001\u3002"}}
{"id": "2511.21691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21691", "abs": "https://arxiv.org/abs/2511.21691", "authors": ["Yusuf Dalva", "Guocheng Gordon Qian", "Maya Goldenberg", "Tsai-Shien Chen", "Kfir Aberman", "Sergey Tulyakov", "Pinar Yanardag", "Kuan-Chieh Jackson Wang"], "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls", "comment": "24 pages; webpage: https://snap-research.github.io/canvas-to-image/", "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.", "AI": {"tldr": "Canvas-to-Image\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u5f02\u6784\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u6587\u672c\u63d0\u793a\u3001\u4e3b\u4f53\u53c2\u8003\u3001\u7a7a\u95f4\u5e03\u5c40\u3001\u59ff\u52bf\u7ea6\u675f\uff09\u7f16\u7801\u6210\u5355\u4e00\u753b\u5e03\u56fe\u50cf\uff0c\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u7ec4\u5408\u751f\u6210\u3002", "motivation": "\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u96be\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7ec4\u5408\u548c\u591a\u6a21\u6001\u63a7\u5236\uff0c\u7279\u522b\u662f\u540c\u65f6\u6307\u5b9a\u591a\u79cd\u63a7\u5236\u65f6\u3002", "method": "\u5c06\u591a\u6837\u63a7\u5236\u4fe1\u53f7\u7f16\u7801\u4e3a\u5355\u4e00\u590d\u5408\u753b\u5e03\u56fe\u50cf\uff1b curation \u591a\u4efb\u52a1\u6570\u636e\u96c6\uff1b\u63d0\u51faMulti-Task Canvas Training\u7b56\u7565\uff0c\u5728\u7edf\u4e00\u8303\u5f0f\u4e0b\u8054\u5408\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7406\u89e3\u5e76\u6574\u5408\u5f02\u6784\u63a7\u5236\u3002", "result": "\u5728\u8eab\u4efd\u4fdd\u5b58\u548c\u63a7\u5236\u9075\u5b88\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u6db5\u76d6\u591a\u4eba\u7269\u7ec4\u5408\u3001\u59ff\u52bf\u63a7\u5236\u7ec4\u5408\u3001\u5e03\u5c40\u7ea6\u675f\u751f\u6210\u548c\u591a\u63a7\u5236\u751f\u6210\u7b49\u6311\u6218\u57fa\u51c6\u3002", "conclusion": "\u5b9e\u73b0\u8de8\u591a\u6a21\u6001\u63a7\u5236\u7684\u63a8\u7406\uff0c\u800c\u975e\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u6cdb\u5316\u826f\u597d\u3002"}}
