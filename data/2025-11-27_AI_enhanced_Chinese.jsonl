{"id": "2511.20710", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20710", "abs": "https://arxiv.org/abs/2511.20710", "authors": ["David Amebley", "Sayanton Dibbo"], "title": "Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?", "comment": null, "summary": "In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.", "AI": {"tldr": "\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u62d3\u6251\u6b63\u5219\u5316\uff08tau\u6846\u67b6\uff09\u63d0\u5347VLMs\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\u7684\u9c81\u68d2\u602724%\uff0c\u4e0d\u663e\u8457\u727a\u7272\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u90e8\u7f72\u589e\u591a\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u4e0a\u5347\uff0c\u73b0\u7814\u7a76\u591a\u805a\u7126\u5355\u6a21\u6001\u7cfb\u7edf\u6216\u5bf9\u6297\u653b\u51fb\uff0c\u795e\u7ecf\u542f\u53d1\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u9690\u79c1\u653b\u51fb\u9c81\u68d2\u6027\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u6027\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u62d3\u6251\u6b63\u5219\u5316tau\u6846\u67b6\uff0c\u5e94\u7528\u4e8eBLIP\u3001PaliGemma 2\u3001ViT-GPT2\u7b49VLMs\uff08NEURO\u53d8\u4f53tau&gt;0\uff09\uff0c\u5728COCO\u3001CC3M\u3001NoCaps\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u9ed1\u76d2MIA\u653b\u51fb\u4e0e\u6a21\u578b\u6548\u7528\uff08MPNet\u3001ROUGE-2\uff09\u3002", "result": "BLIP+COCO\u6570\u636e\u96c6\u4e0a\uff0cNEURO VLM MIA\u6210\u529f\u7387\uff08ROC-AUC\uff09\u5e73\u5747\u4e0b\u964d24%\uff0c\u6548\u7528\u76f8\u4f3c\uff1bPaliGemma 2\u4e0eViT-GPT2\u5728CC3M\u3001NoCaps\u4e0a\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u795e\u7ecf\u542f\u53d1VLMs\u5bf9\u56fe\u50cf-\u6587\u672cMIA\u9690\u79c1\u653b\u51fb\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u6a21\u578b\u9690\u79c1\u98ce\u9669\u7406\u89e3\u65b0\u8bc1\u636e\u3002"}}
{"id": "2511.20714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20714", "abs": "https://arxiv.org/abs/2511.20714", "authors": ["Inferix Team", "Tianyu Feng", "Yizeng Han", "Jiahao He", "Yuanyu He", "Xi Lin", "Teng Liu", "Hanfeng Lu", "Jiasheng Tang", "Wei Wang", "Zhiyuan Wang", "Jichao Wu", "Mingyang Yang", "Yinghao Yu", "Zeyu Zhang", "Bohan Zhuang"], "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "comment": null, "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "AI": {"tldr": "Inferix is a next-generation inference engine for world models using semi-autoregressive decoding to generate high-quality, interactive videos efficiently.", "motivation": "World models need efficient inference for long, realistic videos in AI agents, embodied AI, gaming; semi-autoregressive decoding overcomes limitations of diffusion and autoregressive methods; need for optimized engine beyond high-concurrency or classic diffusion systems.", "method": "Semi-autoregressive (block-diffusion) decoding: generates video tokens in blocks with diffusion within blocks, autoregressive across blocks, using LLM-style KV Cache; Inferix optimizes this with interactive streaming, profiling, and LV-Bench integration.", "result": "Enables efficient, variable-length, high-quality video generation; supports real-time interaction, realistic simulation; provides fine-grained benchmarking via LV-Bench.", "conclusion": "Inferix advances world model inference, distinct from existing systems; calls for community collaboration to foster exploration."}}
{"id": "2511.20720", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20720", "abs": "https://arxiv.org/abs/2511.20720", "authors": ["Haibo HU", "Lianming Huang", "Nan Guan", "Chun Jason Xue"], "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving", "comment": null, "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.", "AI": {"tldr": "DeeAD\u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u3001\u52a8\u4f5c\u5f15\u5bfc\u7684\u65e9\u9000\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u4e2d\u95f4\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u52a0\u901fVLA\u89c4\u5212\uff0c\u5f53\u9884\u6d4b\u8f68\u8ff9\u4e0e\u8f7b\u91cf\u89c4\u5212\u5148\u9a8c\u504f\u5dee\u5c0f\u4e8e2m\u65f6\u63d0\u524d\u7ec8\u6b62\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u8fbe29%\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "VLA\u6a21\u578b\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u4f46\u6df1\u5c42Transformer\u6808\u5bfc\u81f4\u663e\u8457\u63a8\u7406\u5ef6\u8fdf\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u5b9e\u65f6\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u4f5c\u5f15\u5bfc\u65e9\u9000\u800c\u975e\u7f6e\u4fe1\u5206\u6570\uff0c\u5f53\u8f68\u8ff9\u4e0e\u5bfc\u822a\u6216\u4f4e\u7cbe\u5ea6\u89c4\u5212\u5148\u9a8c\u5bf9\u9f50\uff08\u504f\u5dee<2m\uff09\u65f6\u7ec8\u6b62\uff1b\u5f15\u5165\u591a\u8df3\u63a7\u5236\u5668\u6839\u636e\u5206\u6570\u53d8\u5316\u7387\u81ea\u9002\u5e94\u8df3\u8fc7\u5197\u4f59\u5c42\uff1b\u65e0\u7f1d\u96c6\u6210\u73b0\u6709VLA\u6a21\u578b\u5982ORION\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u4e0a\uff0c\u5b9e\u73b028% Transformer\u5c42\u7a00\u758f\u5ea6\u548c29%\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "conclusion": "DeeAD\u6709\u6548\u52a0\u901fVLA\u63a8\u7406\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u727a\u7272\u6027\u80fd\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.20722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20722", "abs": "https://arxiv.org/abs/2511.20722", "authors": ["Minh Thong Doi", "Jan Butora", "Vincent Itier", "J\u00e9r\u00e9mie Boulanger", "Patrick Bas"], "title": "DinoLizer: Learning from the Best for Generative Inpainting Localization", "comment": null, "summary": "We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.", "AI": {"tldr": "\u57fa\u4e8eDINOv2\u7684DinoLizer\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5f0f\u4fee\u590d\u56fe\u50cf\u4e2d\u64cd\u7eb5\u533a\u57df\u7684\u5b9a\u4f4d\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0aIoU\u63d0\u534712%\uff0c\u5bf9\u540e\u5904\u7406\u9c81\u68d2\u3002", "motivation": "\u751f\u6210\u5f0f\u4fee\u590d\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u8bed\u4e49\u64cd\u7eb5\u533a\u57df\u7684\u9700\u6c42\uff0c\u533a\u5206\u8bed\u4e49\u66f4\u6539\u4e0e\u975e\u8bed\u4e49\u7f16\u8f91\uff0c\u5229\u7528DINOv2\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u3002", "method": "\u5728B-Free\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684DINOv2\u57fa\u7840\u4e0a\u6dfb\u52a0\u7ebf\u6027\u5206\u7c7b\u5934\uff0c\u5bf9ViT\u768414\u00d714\u8865\u4e01\u5d4c\u5165\u9884\u6d4b\u64cd\u7eb5\uff1b\u6ed1\u52a8\u7a97\u53e3\u5904\u7406\u5927\u56fe\uff0c\u70ed\u56fe\u540e\u5904\u7406\u751f\u6210\u4e8c\u503c\u63a9\u7801\uff1b\u8bad\u7ec3\u805a\u7126\u8bed\u4e49\u66f4\u6539\u533a\u57df\u3002", "result": "\u5728\u591a\u79cd\u751f\u6210\u6a21\u578b\u7684\u4fee\u590d\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\uff0c\u5e73\u5747IoU\u9ad812%\uff0c\u540e\u5904\u7406\uff08\u5982\u7f29\u653e\u3001\u566a\u58f0\u3001JPEG\u538b\u7f29\uff09\u540e\u4f18\u52bf\u66f4\u5927\uff1bDINOv2\u4f18\u4e8eDINOv3\u7684\u6d88\u878d\u9a8c\u8bc1\u3002", "conclusion": "Vision Transformer\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cDinoLizer\u6027\u80fd\u4f18\u8d8a\uff0c\u4ee3\u7801\u5373\u5c06\u516c\u5f00\u3002"}}
{"id": "2511.20737", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20737", "abs": "https://arxiv.org/abs/2511.20737", "authors": ["Daeheon Jeong", "Seoyeon Byun", "Kihoon Son", "Dae Hyun Kim", "Juho Kim"], "title": "CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design", "comment": null, "summary": "User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCANVAS\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u57fa\u4e8e\u5de5\u5177\u7684\u7528\u6237\u754c\u9762\uff08UI\uff09\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u57fa\u51c6\u5305\u542b598\u4e2a\u4ece3.3K\u79fb\u52a8UI\u8bbe\u8ba1\u4e2d\u91c7\u6837\u7684\u4efb\u52a1\uff0c\u8986\u76d630\u4e2a\u529f\u80fd\u7c7b\u522b\uff0c\u5305\u62ec\u8bbe\u8ba1\u590d\u5236\u548c\u8bbe\u8ba1\u4fee\u6539\u4e24\u7c7b\u4efb\u52a1\u3002VLM\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u5177\u8c03\u7528\u9010\u6b65\u7f16\u8f91\u8bbe\u8ba1\u3002\u5b9e\u9a8c\u663e\u793a\u9886\u5148\u6a21\u578b\u91c7\u7528\u7b56\u7565\u6027\u5de5\u5177\u8c03\u7528\u63d0\u5347\u8bbe\u8ba1\u8d28\u91cf\uff0c\u5e76\u8bc6\u522b\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "UI\u8bbe\u8ba1\u662f\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u5e08\u4f7f\u7528Figma\u7b49\u8f6f\u4ef6\u9010\u6b65\u7cbe\u70bc\u8bbe\u8ba1\u3002VLMs\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u8868\u660e\u5176\u53ef\u64cd\u4f5c\u8bbe\u8ba1\u8f6f\u4ef6\u4e0e\u8bbe\u8ba1\u5e08\u534f\u4f5c\uff0c\u4f46\u7f3a\u4e4f\u8bc4\u4f30\u5de5\u5177\u8bbe\u8ba1\u6027\u80fd\u7684\u57fa\u51c6\uff0c\u5bfc\u81f4\u80fd\u529b\u672a\u77e5\u3002\u5f00\u53d1\u6b64\u7c7b\u57fa\u51c6\u6709\u52a9\u4e8e\u63ed\u793aVLMs\u5728\u4f20\u7edf\u8f6f\u4ef6\u4e2d\u534f\u4f5c\u6f5c\u529b\u3002", "method": "\u5f15\u5165CANVAS\u57fa\u51c6\uff0c\u5305\u542b598\u4e2a\u5de5\u5177\u8bbe\u8ba1\u4efb\u52a1\u53ca ground-truth \u53c2\u8003\uff0c\u4ece30\u529f\u80fd\u7c7b\u522b\uff08\u4f8b\u5982 onboarding\u3001messaging\uff09\u76843.3K\u79fb\u52a8UI\u8bbe\u8ba1\u4e2d\u91c7\u6837\u3002\u6bcf\u4efb\u52a1\u4e2d\uff0cVLM\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u5177\u8c03\u7528\uff08\u5982\u521b\u5efa\u77e9\u5f62\u4f5c\u4e3a\u6309\u94ae\u80cc\u666f\uff09\u9010\u6b65\u66f4\u65b0\u8bbe\u8ba1\uff0c\u4e0e\u8bbe\u8ba1\u8f6f\u4ef6\u94fe\u63a5\u3002\u4efb\u52a1\u5206\u4e3a\u4e24\u7c7b\uff1a(i) \u8bbe\u8ba1\u590d\u5236\uff08\u91cd\u73b0\u6574\u4e2aUI\u5c4f\u5e55\uff09\uff1b(ii) \u8bbe\u8ba1\u4fee\u6539\uff08\u4fee\u6539\u73b0\u6709\u5c4f\u5e55\u7279\u5b9a\u90e8\u5206\uff09\u3002", "result": "\u9886\u5148\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u7b56\u7565\u6027\u7684\u5de5\u5177\u8c03\u7528\uff0c\u4ece\u800c\u6539\u5584\u8bbe\u8ba1\u8d28\u91cf\u3002\u8bc6\u522b\u6a21\u578b\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u589e\u5f3a\u5de5\u5177\u8bbe\u8ba1\u80fd\u529b\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "CANVAS\u57fa\u51c6\u63ed\u793aVLMs\u5728\u5de5\u5177UI\u8bbe\u8ba1\u4e2d\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u5176\u4e0e\u8bbe\u8ba1\u5e08\u534f\u4f5c\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u9519\u8bef\u5206\u6790\u6307\u5bfc\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2511.20770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20770", "abs": "https://arxiv.org/abs/2511.20770", "authors": ["Raghuveer Thirukovalluru", "Xiaochuang Han", "Bhuwan Dhingra", "Emily Dinan", "Maha Elbayad"], "title": "Text-Guided Semantic Image Encoder", "comment": "20 pages, 6 figures", "summary": "Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.", "AI": {"tldr": "\u63d0\u51faText-Guided Semantic Image Encoder (TIE)\uff0c\u4e00\u79cd\u53d7\u6587\u672c\u67e5\u8be2\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u63d0\u5347\u56fe\u50cf\u5230\u6587\u672c\u57fa\u51c6\u6027\u80fd\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u7f16\u7801\u5668\u72ec\u7acb\u9884\u8bad\u7ec3\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6216\u6587\u672c\u67e5\u8be2\u65e0\u611f\u77e5\uff0c\u5bfc\u81f4\u56fe\u50cf\u5904\u7406\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002", "method": "\u63d0\u51faTIE\uff0c\u901a\u8fc7\u8f93\u5165\u6587\u672c\u67e5\u8be2\u6761\u4ef6\u5316\u751f\u6210\u56fe\u50cf\u8868\u793a\uff0c\u63d0\u5347\u7f16\u7801\u5668\u5bf9\u67e5\u8be2\u76f8\u5173\u89c6\u89c9\u7279\u5f81\u7684\u6355\u6349\u3002", "result": "\u57289\u4e2a\u56fe\u50cf\u5230\u6587\u672c\u57fa\u51c6\u4e0a\uff0c1B\u548c3B\u89c4\u6a21\u5e73\u5747\u63d0\u53471.5\u548c1.3\u5206\uff0cDocVQA\u548cInfoVQA\u6700\u9ad8\u8fbe6\u5206\uff1b\u4ec5\u7528\u4e00\u534a\u56fe\u50cf\u74e6\u7247\uff08tokens\uff09\uff0c\u63a8\u7406\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "TIE\u5bf9\u901a\u7528\u67e5\u8be2\u6cdb\u5316\u826f\u597d\uff0c\u5b9a\u6027\u5206\u6790\u663e\u793a\u5176\u5173\u6ce8\u67e5\u8be2\u76f8\u5173\u533a\u57df\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u67e5\u8be2\u7279\u5b9a grounding\u3002"}}
{"id": "2511.20665", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20665", "abs": "https://arxiv.org/abs/2511.20665", "authors": ["Tcharlies Schmitz"], "title": "Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology", "comment": null, "summary": "This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \\r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.", "AI": {"tldr": "Introduces Harmonic Token Projection (HTP), a reversible and deterministic framework for training-free text embeddings using harmonic trajectories derived from Unicode integers, achieving Spearman \u03c1=0.68 on STS-B English with stable multilingual performance and minimal compute.", "motivation": "Motivated by the limitations of neural embeddings that depend on statistical co-occurrence, optimization, training data, vocabularies, and stochastic parameters, HTP aims to create bijective, interpretable mappings from discrete tokens to continuous vectors purely analytically via harmonic geometry.", "method": "HTP encodes each token's Unicode integer representation analytically into a harmonic trajectory, providing phase-coherent projections that are bijective, reversible, and preserve structure; semantic similarity is computed from geometric alignment without any training or stochasticity.", "result": "On Semantic Textual Similarity Benchmark (STS-B), achieves Spearman correlation \u03c1=0.68 in English, maintains stable performance across ten languages, with negligible computational cost and sub-millisecond latency per sentence pair.", "conclusion": "Proves that meaningful semantic relations can emerge from deterministic geometric structures alone, positioning HTP as a transparent, efficient, multilingual alternative to data-driven embeddings."}}
{"id": "2511.20668", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20668", "abs": "https://arxiv.org/abs/2511.20668", "authors": ["Yongfu Xue"], "title": "PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation", "comment": null, "summary": "Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.", "AI": {"tldr": "PIRA\uff1a\u4e00\u79cd\u9488\u5bf9LLM\u5956\u52b1\u6a21\u578b\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u504f\u597d\u6307\u4ee4\u91cd\u6784\u3001\u591a\u6837\u4efb\u52a1\u5956\u52b1\u805a\u5408\u548c\u53d8dropout\u503c\u5934\u5e73\u5747\uff0c\u89e3\u51b3\u6570\u636e\u6548\u7387\u4f4e\u548c\u8fc7\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5bf9\u9f50LLM\u4e0e\u4eba\u7c7b\u504f\u597d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5224\u522b\u5f0f\u6a21\u578b\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a(1) \u76f4\u63a5\u62fc\u63a5\u95ee\u7b54\u8f93\u5165\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\uff1b(2) \u6613\u53d7\u5956\u52b1\u8fc7\u4f18\u5316\u5f71\u54cd\u3002", "method": "\u63d0\u51faPIRA\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u62ec\u4e09\u7b56\u7565\uff1a(1) \u5c06\u95ee\u7b54\u5bf9\u91cd\u6784\u4e3a\u660e\u786e\u504f\u597d\u6307\u4ee4\uff1b(2) \u805a\u5408\u591a\u6837\u504f\u597d\u4efb\u52a1\u5956\u52b1\u4ee5\u51cf\u504f\u5e76\u63d0\u5347\u9c81\u68d2\u6027\uff1b(3) \u5728\u4e0d\u540cdropout\u7387\u4e0b\u5e73\u5747\u503c\u5934\u8f93\u51fa\u4ee5\u7a33\u5b9a\u5956\u52b1\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PIRA\u7684\u6709\u6548\u6027\u3002", "conclusion": "PIRA\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2511.20804", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20804", "abs": "https://arxiv.org/abs/2511.20804", "authors": ["Kriti Ghosh", "Devjyoti Chakraborty", "Lakshmish Ramaswamy", "Suchendra M. Bhandarkar", "In Kee Kim", "Nancy O'Hare", "Deepak Mishra"], "title": "$\u0394$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer", "comment": null, "summary": "Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $\u0394$-NeRF, a unique modular residual framework for incremental NeRF refinement. $\u0394$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\\% of original size). Experiments on satellite imagery demonstrate that $\u0394$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\\%. $\u0394$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.", "AI": {"tldr": "\u63d0\u51fa\u0394-NeRF\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u6b8b\u5dee\u6846\u67b6\uff0c\u7528\u4e8eNeRF\u7684\u589e\u91cf\u7cbe\u70bc\uff0c\u65e0\u9700\u8fc7\u53bb\u6570\u636e\u5373\u53ef\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u652f\u6301\u536b\u661f\u5730\u5f62\u5206\u6790\u7b49\u987a\u5e8f\u6570\u636e\u573a\u666f\u3002\u901a\u8fc7\u6b8b\u5dee\u63a7\u5236\u5668\u3001\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u3001\u89c6\u56fe\u9009\u62e9\uff08\u51cf\u5c1147%\u6570\u636e\uff09\u548c\u77e5\u8bc6\u84b8\u998f\uff08\u538b\u7f29\u81f320%\u5927\u5c0f\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u7cbe\u70bc\u3002", "motivation": "\u73b0\u6709NeRF\u5728\u5f15\u5165\u65b0\u89c6\u56fe\u65f6\u9700\u5b8c\u6574\u91cd\u8bad\uff0c\u9650\u5236\u4e86\u987a\u5e8f\u6570\u636e\u573a\u666f\uff08\u5982\u536b\u661f\u53cd\u590d\u89c2\u6d4b\u5730\u5f62\uff09\u7684\u5e94\u7528\uff1b\u589e\u91cf\u7cbe\u70bc\u7814\u7a76\u4e0d\u8db3\uff0c\u6734\u7d20\u65b9\u6cd5\u6613\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u0394-NeRF\u6846\u67b6\uff1a(1) \u6b8b\u5dee\u63a7\u5236\u5668\u5411\u51bb\u7ed3\u57faNeRF\u7684\u6bcf\u5c42\u6ce8\u5165\u4fee\u6b63\uff1b(2) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u57fa\u9884\u6d4b\u4e0e\u7cbe\u70bc\u9884\u6d4b\uff0c\u907f\u514d\u8fc7\u5ea6\u4fee\u6b63\uff1b(3) \u89c6\u56fe\u9009\u62e9\u7b56\u7565\u51cf\u5c11\u8bad\u7ec3\u6570\u636e47%\uff1b\u9644\u52a0\u77e5\u8bc6\u84b8\u998f\u538b\u7f29\u6a21\u578b\u81f3\u539f20%\u5927\u5c0f\u3002", "result": "\u536b\u661f\u56fe\u50cf\u5b9e\u9a8c\u663e\u793a\uff0c\u0394-NeRF\u6027\u80fd\u5ab2\u7f8e\u8054\u5408\u8bad\u7ec3\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1130-42%\uff1b\u4f18\u4e8e\u57fa\u7ebf\uff0cPSNR\u8f83\u6734\u7d20\u5fae\u8c03\u63d0\u5347\u81f343.5%\uff0c\u67d0\u4e9b\u6307\u6807\u8d85\u8054\u5408\u8bad\u7ec3\u3002", "conclusion": "\u0394-NeRF\u63d0\u4f9b\u65e0\u9700\u8fc7\u53bb\u6570\u636e\u7684\u9ad8\u6548NeRF\u589e\u91cf\u7cbe\u70bc\u65b9\u6848\uff0c\u5728\u987a\u5e8f\u6570\u636e\u9886\u57df\u5982\u536b\u661f\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u4e0e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2511.20669", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20669", "abs": "https://arxiv.org/abs/2511.20669", "authors": ["Mann Khatri", "Mirza Yusuf", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "title": "Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data", "comment": "Accepted at BDA 2025 as short paper; This paper is long version", "summary": "Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u672c\u6587\u901a\u8fc7\u4e09\u79cd\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u6539\u5584\uff1a\u5728\u4e09\u4e2a\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u5305\u62ec\u6309\u4fee\u8f9e\u89d2\u8272\u91cd\u7ec4\u6587\u6863\u3001\u5b9a\u4e49\u4fee\u8f9e\u89d2\u8272\u4ee5\u719f\u6089\u6cd5\u5f8b\u672f\u8bed\uff0c\u4ee5\u53ca\u6a21\u62df\u6cd5\u9662\u9010\u6b65\u63a8\u7406\uff0cF1\u5206\u6570\u63d0\u53471.5%~4.36%\u3002", "motivation": "LLMs\u867d\u5177\u5907\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u6cd5\u5f8b\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\uff0c\u4e14\u6cd5\u5f8b\u6587\u6863\u5197\u957f\u590d\u6742\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5168\u6587\uff1b\u4ee5\u5f80\u7814\u7a76\u4f7f\u7528\u4e0a\u4e0b\u6587\u65b9\u6cd5\u586b\u8865\u77e5\u8bc6\u7f3a\u53e3\uff0c\u672c\u6587\u9488\u5bf9\u6cd5\u5f8b\u4efb\u52a1\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5728\u4e09\u4e2a\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u5f00\u5c55\u4e09\u7c7b\u5b9e\u9a8c\uff1a(i) \u6309\u4fee\u8f9e\u89d2\u8272\u91cd\u7ec4\u6587\u6863\uff0c\u8bc4\u4f30\u7ed3\u6784\u5316\u4fe1\u606f\u5bf9\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u548c\u51b3\u7b56\u7684\u5f71\u54cd\uff1b(ii) \u5b9a\u4e49\u4fee\u8f9e\u89d2\u8272\uff0c\u5e2e\u52a9\u6a21\u578b\u719f\u6089\u6cd5\u5f8b\u672f\u8bed\uff1b(iii) \u6a21\u62df\u6cd5\u9662\u5bf9\u4fee\u8f9e\u89d2\u8272\u7684\u9010\u6b65\u63a8\u7406\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u7ec4\u7ec7\u6570\u636e\u6216\u89e3\u91ca\u5173\u952e\u6cd5\u5f8b\u672f\u8bed\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cF1\u5206\u6570\u76f8\u6bd4\u57fa\u7ebf\u6700\u4f4e\u63d0\u5347\u7ea61.5%\uff0c\u6700\u9ad84.36%\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u65b9\u6cd5\u5982\u6587\u6863\u91cd\u7ec4\u3001\u672f\u8bed\u89e3\u91ca\u548c\u63a8\u7406\u6a21\u62df\u6709\u6548\u63d0\u5347LLMs\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5b8c\u6574\u9886\u57df\u5bf9\u9f50\u3002"}}
{"id": "2511.20809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20809", "abs": "https://arxiv.org/abs/2511.20809", "authors": ["Ozgur Kara", "Yujia Chen", "Ming-Hsuan Yang", "James M. Rehg", "Wen-Sheng Chu", "Du Tran"], "title": "Layer-Aware Video Composition via Split-then-Merge", "comment": "Project Webpage: https://split-then-merge.github.io", "summary": "We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io", "code_url": "https://split-then-merge.github.io", "AI": {"tldr": "\u63d0\u51faSplit-then-Merge (StM)\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u751f\u6210\u89c6\u9891\u7ec4\u5408\u7684\u63a7\u5236\u6027\u5e76\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u65e0\u6807\u7b7e\u89c6\u9891\u62c6\u5206\u4e3a\u52a8\u6001\u524d\u666f\u548c\u80cc\u666f\u5c42\uff0c\u5e76\u81ea\u7ec4\u5408\u5b66\u4e60\u52a8\u6001\u4e3b\u4f53\u4e0e\u573a\u666f\u4ea4\u4e92\uff0c\u5b9e\u73b0\u771f\u5b9e\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u751f\u6210\u89c6\u9891\u7ec4\u5408\u7f3a\u4e4f\u63a7\u5236\uff0c\u4e14\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96c6\u6216\u624b\u5de5\u89c4\u5219\uff1bStM\u5229\u7528\u65e0\u6807\u7b7e\u89c6\u9891\u5927\u89c4\u6a21\u81ea\u7ec4\u5408\uff0c\u5b66\u4e60\u590d\u6742\u7ec4\u5408\u52a8\u6001\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u5c06\u65e0\u6807\u7b7e\u89c6\u9891\u62c6\u5206\u4e3a\u52a8\u6001\u524d\u666f/\u80cc\u666f\u5c42\uff0c\u81ea\u7ec4\u5408\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff1b\u5f15\u5165\u53d8\u6362\u611f\u77e5\u8bad\u7ec3\u7ba1\u9053\uff0c\u5305\u62ec\u591a\u5c42\u878d\u5408\u4e0e\u589e\u5f3a\u5b9e\u73b0\u53ef\u4f9b\u6027\u611f\u77e5\u7ec4\u5408\uff0c\u4ee5\u53ca\u8eab\u4efd\u4fdd\u6301\u635f\u5931\u786e\u4fdd\u524d\u666f\u4fdd\u771f\u3002", "result": "\u5728\u5b9a\u91cf\u57fa\u51c6\u548c\u4eba\u7c7b/VLLM\u5b9a\u6027\u8bc4\u4f30\u4e2d\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002", "conclusion": "StM\u6709\u6548\u63d0\u5347\u751f\u6210\u89c6\u9891\u7ec4\u5408\u6027\u80fd\uff0c\u9879\u76ee\u9875\u9762\u8be6\u89c1https://split-then-merge.github.io\u3002"}}
{"id": "2511.20672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20672", "abs": "https://arxiv.org/abs/2511.20672", "authors": ["Saad Mankarious", "Ayah Zirikly", "Daniel Wiechmann", "Elma Kerz", "Edward Kempa", "Yu Qiao"], "title": "MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data", "comment": null, "summary": "Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \\textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \\textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \\textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.", "AI": {"tldr": "New Reddit benchmark dataset MindSET with over 13M annotated posts across 7 mental health conditions, rigorously preprocessed (language filter, NSFW/duplicate removal), LIWC analysis, outperforms prior benchmarks by up to 18 F1 points in diagnosis detection.", "motivation": "Existing benchmarks for social media mental health analysis are outdated due to limited data, inadequate cleaning, and content diversity (multilingual, harmful material), missing real-time insights.", "method": "Curated from Reddit using self-reported diagnoses; annotated 13M+ posts; preprocessing: language filtering, NSFW/duplicate removal; LIWC linguistic analysis; binary classification with fine-tuned LMs and BoW features.", "result": "Models on MindSET outperform prior benchmarks, achieving up to 18-point F1 improvement for Autism detection.", "conclusion": "MindSET provides a robust foundation for mental health research from social media, supporting early risk detection and psychological trend analysis."}}
{"id": "2511.20814", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20814", "abs": "https://arxiv.org/abs/2511.20814", "authors": ["Md Tanvirul Alam", "Saksham Aggarwal", "Justin Yang Chae", "Nidhi Rastogi"], "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning", "comment": null, "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.", "AI": {"tldr": "Sphinx\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u7684\u5408\u6210\u73af\u5883\u57fa\u51c6\uff0c\u6db5\u76d625\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u751f\u6210\u5e26\u6709\u53ef\u9a8c\u8bc1ground-truth\u7684\u8c1c\u9898\u3002\u9876\u7ea7LVLMs\u5982GPT-5\u51c6\u786e\u7387\u4ec551.1%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff1bRLVR\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u5916\u90e8\u57fa\u51c6\u4e0a\u83b7\u76ca\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u6838\u5fc3\u8ba4\u77e5\u539f\u8bed\uff08\u5982\u5bf9\u79f0\u3001\u51e0\u4f55\u53d8\u6362\u3001\u7a7a\u95f4\u63a8\u7406\u7b49\uff09\u4e0a\u7684\u8bc4\u4f30\u9700\u6c42\uff0c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u7cbe\u786e\u6027\u548c\u5927\u89c4\u6a21\u6027\uff0c\u73b0\u5f15\u5165Sphinx\u4ee5\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "Sphinx\u4f7f\u7528\u56fe\u6848\u3001\u56fe\u5757\u3001\u56fe\u8868\u3001\u56fe\u6807\u548c\u51e0\u4f55\u539f\u8bed\u8fc7\u7a0b\u5316\u751f\u6210\u8c1c\u9898\uff0c\u8986\u76d625\u79cd\u4efb\u52a1\uff08\u5bf9\u79f0\u68c0\u6d4b\u3001\u51e0\u4f55\u53d8\u6362\u3001\u7a7a\u95f4\u63a8\u7406\u3001\u56fe\u8868\u89e3\u8bfb\u3001\u5e8f\u5217\u9884\u6d4b\uff09\uff0c\u914d\u4ee5ground-truth\uff1b\u8bc4\u4f30LVLMs\uff0c\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u9876\u7ea7LVLMs\u5982GPT-5\u5728Sphinx\u4e0a\u51c6\u786e\u7387\u4ec551.1%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff1bRLVR\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728Sphinx\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5916\u90e8\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u4ea7\u751f\u6b63\u8fc1\u79fb\u3002", "conclusion": "Sphinx\u7a81\u663eLVLMs\u5728\u57fa\u7840\u89c6\u89c9\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0cRLVR\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u5c55\u793a\u51fa\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.20673", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20673", "abs": "https://arxiv.org/abs/2511.20673", "authors": ["Zheng Hui", "Xiaokai Wei", "Reza Shirkavand", "Chen Wang", "Weizhi Zhang", "Alejandro Pel\u00e1ez", "Michelle Gong"], "title": "Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation", "comment": null, "summary": "Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.", "AI": {"tldr": "\u751f\u6210\u63a8\u8350\u65b0\u8303\u5f0fFlexCode\uff0c\u901a\u8fc7\u6d41\u884c\u5ea6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u7801\u672c\u5206\u914d\uff08CF\u7801\u672c\u4e0e\u8bed\u4e49\u7801\u672c\uff09\uff0c\u5e73\u8861\u70ed\u95e8\u4e0e\u957f\u5c3e\u7269\u54c1\u8868\u793a\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u5c3e\u90e8\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7edf\u4e00\u7801\u672c\u7f16\u7801\u6240\u6709\u7269\u54c1\uff0c\u5ffd\u7565\u70ed\u95e8\u7269\u54c1\uff08\u534f\u4f5c\u4fe1\u53f7\u4e30\u5bcc\uff09\u4e0e\u957f\u5c3e\u7269\u54c1\uff08\u4f9d\u8d56\u8bed\u4e49\u7406\u89e3\uff09\u7684\u5931\u8861\uff0c\u5bfc\u81f4\u8868\u793a\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faFlexCode\u6846\u67b6\uff1a\u6d41\u884c\u5ea6\u611f\u77e5\uff0c\u81ea\u9002\u5e94\u5206\u914d\u56fa\u5b9a\u4ee4\u724c\u9884\u7b97\u81f3\u534f\u4f5c\u8fc7\u6ee4\uff08CF\uff09\u7801\u672c\u548c\u8bed\u4e49\u7801\u672c\uff1b\u8f7b\u91cfMoE\u52a8\u6001\u5e73\u8861CF\u7cbe\u5ea6\u4e0e\u8bed\u4e49\u6cdb\u5316\uff1b\u5bf9\u9f50\u4e0e\u5e73\u6ed1\u76ee\u6807\u786e\u4fdd\u8de8\u6d41\u884c\u5ea6\u8c31\u7cfb\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516c\u5171\u548c\u5de5\u4e1a\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cFlexCode\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u957f\u5c3e\u9c81\u68d2\u6027\u3002", "conclusion": "FlexCode\u4e3a\u751f\u6210\u63a8\u8350\u5668\u63d0\u4f9b\u4ee4\u724c\u8868\u793a\u65b0\u673a\u5236\uff0c\u5e73\u8861\u8bb0\u5fc6\u4e0e\u6cdb\u5316\uff0c\u5e76\u5e26\u6765\u5bf9\u57fa\u4e8e\u4ee4\u724c\u63a8\u8350\u6a21\u578b\u7684\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.20677", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20677", "abs": "https://arxiv.org/abs/2511.20677", "authors": ["Saleh Almohaimeed", "May Alsofyani", "Saad Almohaimeed", "Mansour Al Ghanim", "Liqiang Wang"], "title": "Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic", "comment": "Accepted at IJCNN 2025 (to appear in IEEE/IJCNN proceedings). This arXiv submission corresponds to the camera-ready version", "summary": "In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u8de8\u57df\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684text-to-SQL\u6570\u636e\u96c6Ar-SParC\uff0c\u5305\u542b3450\u4e2a\u5e8f\u5217\uff08\u603b10225\u4e2a\u95ee\u9898\uff09\u3002\u4f7f\u7528GPT-3.5-turbo\u548cGPT-4.5-turbo\u8fdb\u884c40\u4e2a\u5b9e\u9a8c\uff0c\u6d4b\u8bd510\u79cd\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff084\u79cd\u95ee\u9898\u8868\u793a\u6cd5+6\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u3002\u63d0\u51fa\u65b0\u578bGAT corrector\uff0c\u63d0\u5347\u96f6\u6837\u672cEX/IX\u51c6\u786e\u7387\u5e73\u57471.9%\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0bEX 1.72%\u3001IX 0.92%\u3002\u6d88\u878d\u5b9e\u9a8c\u89e3\u91ca\u5176\u4f18\u4e8eGAT verifier\u7684\u539f\u56e0\uff0c\u5c24\u5176\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56text-to-SQL\u6570\u636e\u96c6\u548c\u7814\u7a76\u4e3b\u8981\u9650\u4e8e\u82f1\u8bed\u53ca\u5c11\u91cf\u4e2d\u6587\uff0c\u963f\u62c9\u4f2f\u8bed\u9886\u57df\u5c1a\u65e0\u76f8\u5173\u5de5\u4f5c\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u3002", "method": "\u6784\u5efaAr-SParC\u6570\u636e\u96c6\uff1a3450\u5e8f\u5217\u3001\u5e73\u57473\u95ee\u3001\u603b10225\u95ee\u53ca\u5bf9\u5e94SQL\u3002\u5f00\u5c5540\u5b9e\u9a8c\uff1a2\u4e2a\u5927\u6a21\u578b\uff08GPT-3.5-turbo\u3001GPT-4.5-turbo\uff09\u300110\u79cd\u63d0\u793a\u6280\u672f\uff084\u95ee\u9898\u8868\u793a+6 in-context learning\uff09\u3002\u5f00\u53d1GAT corrector\u8fdb\u884cSQL\u4fee\u6b63\uff0c\u8fdb\u884c2\u4e2a\u6d88\u878d\u5b9e\u9a8c\u5bf9\u6bd4GAT verifier\u3002", "result": "GAT corrector\u5728\u6240\u670940\u5b9e\u9a8c\u4e2d\u63d0\u5347\u6027\u80fd\uff1a\u96f6\u6837\u672c\u5e73\u5747EX +1.9%\u3001IX +1.9%\uff1bin-context\u5b66\u4e60EX +1.72%\u3001IX +0.92%\u3002\u6d88\u878d\u663e\u793a\u5176\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\u4f18\u4e8eGAT verifier\u3002", "conclusion": "Ar-SParC\u4e3a\u963f\u62c9\u4f2f\u8bedtext-to-SQL\u63d0\u4f9b\u9996\u4e2a\u6570\u636e\u96c6\uff0cGAT corrector\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u9610\u660e\u5176\u4f18\u52bf\uff0c\u7279\u522b\u662f\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u6311\u6218\u3002"}}
{"id": "2511.20823", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20823", "abs": "https://arxiv.org/abs/2511.20823", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alv\u00e9n", "Fredrik Kahl"], "title": "RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs", "comment": null, "summary": "Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.", "AI": {"tldr": "RefTr\u662f\u4e00\u79cd3D\u56fe\u50cf\u5230\u56fe\u6a21\u578b\uff0c\u7528\u4e8e\u8840\u7ba1\u6811\u4e2d\u5fc3\u7ebf\u751f\u6210\uff0c\u901a\u8fc7Transformer\u89e3\u7801\u5668\u7684Producer-Refiner\u67b6\u6784\uff0c\u5bf9\u521d\u59cb\u6c47\u5408\u8f68\u8ff9\u8fdb\u884c\u9012\u5f52\u7cbe\u70bc\uff0c\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387\u548c\u9ad8\u6548\u62d3\u6251\u6b63\u786e\u7684\u6811\u72b6\u4e2d\u5fc3\u7ebf\u63d0\u53d6\u3002", "motivation": "\u7ba1\u72b6\u6811\u7ed3\u6784\uff08\u5982\u8840\u7ba1\u548c\u80ba\u6c14\u9053\uff09\u5bf9\u4eba\u4f53\u7269\u8d28\u8fd0\u8f93\u81f3\u5173\u91cd\u8981\uff0c\u51c6\u786e\u68c0\u6d4b\u5176\u4e2d\u5fc3\u7ebf\u5e76\u4fdd\u6301\u6b63\u786e\u6811\u62d3\u6251\u5bf9\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u624b\u672f\u5bfc\u822a\u7b49\u4e34\u5e8a\u4efb\u52a1\u5173\u952e\uff0c\u9ad8\u53ec\u56de\u7387\u5c24\u4e3a\u91cd\u8981\uff0c\u907f\u514d\u9057\u6f0f\u5c0f\u5206\u652f\u5bfc\u81f4\u81f4\u547d\u9519\u8bef\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u89e3\u7801\u5668\u7684Producer-Refiner\u67b6\u6784\uff1aProducer\u63d0\u51fa\u521d\u59cb\u6c47\u5408\u8f68\u8ff9\u96c6\uff0cRefiner\u9012\u5f52\u7cbe\u70bc\u4ea7\u751f\u6700\u7ec8\u8f68\u8ff9\u5f62\u6210\u4e2d\u5fc3\u7ebf\u56fe\uff1b\u6c47\u5408\u8f68\u8ff9\u8868\u793a\u786e\u4fdd\u5b8c\u6574\u8f68\u8ff9\u7cbe\u70bc\u5e76\u5f3a\u5236\u6709\u6548\u6811\u62d3\u6251\uff1b\u5f15\u5165\u9ad8\u6548\u7a7a\u95f4\u6811\u56fe\u975e\u6700\u5927\u6291\u5236\u7b97\u6cd5\u5408\u5e76\u91cd\u590d\u5206\u652f\uff1b\u9012\u5f52\u7cbe\u70bc\u63d0\u5347\u7cbe\u5ea6\u5e76\u51cf\u5c112.4\u500d\u89e3\u7801\u5668\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u4e2d\u5fc3\u7ebf\u6570\u636e\u96c6\u4e0a\uff0cRefTr\u5b9e\u73b0\u4f18\u4e8eSOTA\u7684\u53ec\u56de\u7387\u3001\u76f8\u5f53\u7cbe\u5ea6\u3001\u66f4\u5feb\u63a8\u7406\u901f\u5ea6\u548c\u663e\u8457\u66f4\u5c11\u53c2\u6570\u3002", "conclusion": "RefTr\u4f5c\u4e3a\u8840\u7ba1\u68113D\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u65b0SOTA\u6846\u67b6\uff0c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.20680", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20680", "abs": "https://arxiv.org/abs/2511.20680", "authors": ["Matthew W. Kenaston", "Umair Ayub", "Mihir Parmar", "Muhammad Umair Anjum", "Syed Arsalan Ahmed Naqvi", "Priya Kumar", "Samarth Rawal", "Aadel A. Chaudhuri", "Yousef Zakharia", "Elizabeth I. Heath", "Tanios S. Bekaii-Saab", "Cui Tao", "Eliezer M. Van Allen", "Ben Zhou", "YooJung Choi", "Chitta Baral", "Irbaz Bin Riaz"], "title": "Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes", "comment": "24 pages, 6 figures, 1 supplementary figure, 3 tables", "summary": "Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u80bf\u7624\u5b66\u4e34\u5e8a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53ef\u80fd\u901a\u8fc7\u9519\u8bef\u63a8\u7406\u5f97\u51fa\u6b63\u786e\u7ed3\u8bba\uff0c\u6b64\u5b89\u5168\u9690\u60a3\u672a\u88ab\u51c6\u786e\u7387\u8bc4\u4f30\u6355\u6349\u3002\u672c\u7814\u7a76\u5f00\u53d1\u63a8\u7406\u9519\u8bef\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u5176\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u57fa\u51c6\u6027\u80fd\u9ad8\uff0c\u4f46\u63a8\u7406\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u80bf\u7624\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u4e34\u5e8a\u4e0d\u5b89\u5168\u63a8\u8350\uff0c\u9700\u5f00\u53d1\u6355\u6349\u63a8\u7406\u5931\u8d25\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4e24\u961f\u5217\u56de\u987e\u6027\u7814\u7a76\uff1a\u4f7f\u7528CORAL\u6570\u636e\u96c6\u4e73\u817a\u764c\u548c\u80f0\u817a\u764c\u7b14\u8bb0\uff0c\u6807\u6ce8GPT-4\u601d\u7ef4\u94fe600\u6761\u54cd\u5e94\uff0c\u5b9a\u4e49\u4e09\u7ea7\u5206\u7c7b\u6cd5\u6620\u5c04\u8ba1\u7b97\u5931\u8d25\u81f3\u8ba4\u77e5\u504f\u5dee\u6846\u67b6\uff1b\u5728822\u6761\u524d\u5217\u817a\u764c\u54a8\u8be2\u7b14\u8bb0\u54cd\u5e94\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u62df\u63d0\u53d6\u3001\u5206\u6790\u548c\u4e34\u5e8a\u63a8\u8350\u4efb\u52a1\u3002", "result": "23%\u89e3\u91ca\u5b58\u5728\u63a8\u7406\u9519\u8bef\uff0c\u4e3b\u5bfc\u6574\u4f53\u9519\u8bef\uff0c\u4ee5\u786e\u8ba4\u504f\u5dee\u548c\u951a\u5b9a\u504f\u5dee\u6700\u5e38\u89c1\uff1b\u63a8\u7406\u5931\u8d25\u4e0e\u6307\u5357\u4e0d\u4e00\u81f4\u53ca\u6f5c\u5728\u6709\u5bb3\u63a8\u8350\u76f8\u5173\uff0c\u5c24\u5176\u665a\u671f\u75be\u75c5\u7ba1\u7406\uff1b\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u8bc4\u4f30\u5668\u80fd\u68c0\u6d4b\u9519\u8bef\u5b58\u5728\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u5206\u7c7b\u5b50\u7c7b\u578b\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7455\u75b5\u53ef\u80fd\u4ea7\u751f\u6d41\u5229\u4f46\u4e34\u5e8a\u4e0d\u5b89\u5168\u7684\u63a8\u8350\uff1b\u6240\u63d0\u5206\u7c7b\u6cd5\u4e3a\u4e34\u5e8a\u90e8\u7f72\u524d\u8bc4\u4f30\u548c\u63d0\u5347\u63a8\u7406\u4fdd\u771f\u5ea6\u63d0\u4f9b\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2511.20853", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.20853", "abs": "https://arxiv.org/abs/2511.20853", "authors": ["Nisarg K. Trivedi", "Vinayak A. Belludi", "Li-Yun Wang", "Pardis Taghavi", "Dante Lok"], "title": "MODEST: Multi-Optics Depth-of-Field Stereo Dataset", "comment": null, "summary": "Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9ad8\u5206\u8fa8\u7387\uff085472\u00d73648px\uff09\u771f\u5b9e\u7acb\u4f53DSLR\u6570\u636e\u96c6\uff0c\u5305\u542b18000\u5f20\u56fe\u50cf\uff0c\u7cfb\u7edf\u53d8\u7126\u8ddd\uff0828-70mm\uff09\u548c\u5149\u5708\uff08f/2.8-f/22\uff09\uff0c\u8986\u76d69\u4e2a\u590d\u6742\u771f\u5b9e\u573a\u666f\uff0c\u7528\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u3001\u666f\u6df1\u6e32\u67d3\u7b49\u4efb\u52a1\u8bc4\u4f30\u4e0e\u6cdb\u5316\u7814\u7a76\u3002", "motivation": "\u771f\u5b9e\u5149\u5b66\u6761\u4ef6\u4e0b\u53ef\u9760\u6df1\u5ea6\u4f30\u8ba1\u4ecd\u662f\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u7814\u7a76\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u771f\u5b9e\u7acb\u4f53DSLR\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u4e0e\u8bc4\u4f30\u56f0\u96be\u3002", "method": "\u9488\u5bf99\u4e2a\u590d\u6742\u5ea6\u3001\u7167\u660e\u3001\u80cc\u666f\u5404\u5f02\u7684\u573a\u666f\uff0c\u4f7f\u7528\u4e24\u4e2a\u76f8\u540c\u76f8\u673a\u7ec4\u4ef6\uff0c\u572810\u4e2a\u7126\u8ddd\u548c5\u4e2a\u5149\u5708\u4e0b\u6355\u83b7\u6bcf\u573a\u666f2000\u5f20\u56fe\u50cf\uff08\u603b18000\u5f20\uff09\uff0c\u5168\u8986\u76d650\u79cd\u5149\u5b66\u914d\u7f6e\uff1b\u6bcf\u4e2a\u914d\u7f6e\u9644\u4e13\u7528\u6821\u51c6\u56fe\u50cf\u96c6\uff1b\u573a\u666f\u5305\u542b\u591a\u5c3a\u5ea6\u5149\u5b66\u9519\u89c9\u3001\u53cd\u5c04/\u900f\u660e\u8868\u9762\u3001\u7ec6\u7c92\u7ec6\u8282\u53ca\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u5143\u7d20\u3002", "result": "\u53d1\u5e03\u6570\u636e\u96c6\u3001\u6821\u51c6\u6587\u4ef6\u53ca\u8bc4\u4f30\u4ee3\u7801\uff1b\u5c55\u793a\u5f53\u524d\u5355\u76ee/\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u3001\u666f\u6df1\u6e32\u67d3\u7b49SOTA\u65b9\u6cd5\u5728\u771f\u5b9e\u5149\u5b66\u4e0b\u7684\u6311\u6218\uff0c\u652f\u6301\u51e0\u4f55/\u5149\u5b66\u6548\u5e94\u5206\u6790\u3001\u53bb\u6a21\u7cca\u30013D\u91cd\u5efa\u53ca\u65b0\u89c6\u56fe\u5408\u6210\u7b49\u3002", "conclusion": "\u6865\u63a5\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u76f8\u673a\u5149\u5b66\u95f4\u7684\u73b0\u5b9e\u5dee\u8ddd\uff0c\u63a8\u52a8\u53ef\u590d\u73b0\u7684\u771f\u5b9e\u4e16\u754c\u5149\u5b66\u6cdb\u5316\u7814\u7a76\u3002"}}
{"id": "2511.20683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20683", "abs": "https://arxiv.org/abs/2511.20683", "authors": ["Bharadwaj Yadavalli"], "title": "Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches", "comment": "20 pages, 4 figures, includes production-scale experiments across OpenAI GPT-4, Google Gemini, and Anthropic Claude; code available upon request", "summary": "Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.\n  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.\n  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6a21\u677f\u9009\u62e9\uff08DTS\uff09\uff0c\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u54cd\u5e94\u6a21\u677f\uff0c\u663e\u8457\u964d\u4f4e\u4ee4\u724c\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5404\u79cd\u67e5\u8be2\u4f7f\u7528\u7edf\u4e00\u5197\u957f\u54cd\u5e94\u7b56\u7565\uff0c\u5bfc\u81f4\u4ee4\u724c\u4f4e\u6548\uff0c\u5c24\u5176\u662f\u8f93\u51fa\u4ee4\u724c\u4ef7\u683c\u4e3a\u8f93\u5165\u76844-8\u500d\u3002", "method": "\u5f00\u53d1DTS\uff0c\u4f7f\u7528\u7b80\u5355MLP\uff08\u57fa\u4e8e\u9884\u8ba1\u7b97\u5d4c\u5165\uff09\u6216\u5fae\u8c03RoBERTa\u4f5c\u4e3a\u8def\u7531\u5668\uff0c\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u9009\u62e9\u6a21\u677f\uff1b\u57281000\u4e2aMMLU\u95ee\u9898\u4e0a\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc79000\u4e2a\u751f\u4ea7API\u8c03\u7528\u8de8OpenAI GPT-4\u3001Google Gemini\u548cAnthropic Claude\u9a8c\u8bc1\u3002", "result": "MLP\u8def\u7531\u51c6\u786e\u7387\u8fbe90.5%\uff0c\u7565\u9ad8\u4e8eRoBERTa\uff0889.5%\uff09\uff0c\u53c2\u6570\u5c111.25\u4ebf\uff1b\u8de8\u63d0\u4f9b\u5546\u51c6\u786e\u7387\u4e00\u81f4\uff0c\u4ee4\u724c\u51cf\u5c1132.6%-33.9%\u3002", "conclusion": "\u8d21\u732e\u5305\u62ec\uff1a\u673a\u5668\u5b66\u4e60\u7406\u8bba\u57fa\u7840\u4e0b\u7684\u5f62\u5f0f\u5316\u95ee\u9898\u8868\u8ff0\u3001\u56db\u4e2a\u7b97\u6cd5\u53ca\u5176\u590d\u6742\u5ea6\u5206\u6790\u3001\u751f\u4ea7\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2511.20854", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20854", "abs": "https://arxiv.org/abs/2511.20854", "authors": ["Sree Bhattacharyya", "Yaman Kumar Singla", "Sudhir Yarram", "Somesh Kumar Singh", "Harini S", "James Z. Wang"], "title": "Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries", "comment": "Accepted at WACV 2026", "summary": "Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.", "AI": {"tldr": "\u5f15\u5165\u9996\u4e2a\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u89c6\u89c9\u8bb0\u5fc6\u6570\u636e\u96c6\uff0c\u542b\u8d8582,000\u4e2a\u89c6\u9891\u53caReddit ToT\u67e5\u8be2\u7684\u56de\u5fc6\u63cf\u8ff0\uff0c\u652f\u6301\u56de\u5fc6\u751f\u6210\u548cToT\u68c0\u7d22\u4efb\u52a1\uff0c\u5fae\u8c03VLMs\u8d85\u8d8aGPT-4o\u3002", "motivation": "\u89c6\u89c9\u5185\u5bb9\u8bb0\u5fc6\u6027\u7814\u7a76\u56e0\u4eba\u7c7b\u6807\u6ce8\u6210\u672c\u9ad8\u800c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\u53d7\u9650\uff0c\u73b0\u6709\u7684\u8bb0\u5fc6\u5206\u6570\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u5f00\u653e\u56de\u5fc6\u63cf\u8ff0\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u3002", "method": "\u5229\u7528Reddit\u7b49\u5e73\u53f0\u7684ToT\u68c0\u7d22\u67e5\u8be2\u6536\u96c6\u65e0\u76d1\u7763\u56de\u5fc6\u6570\u636e\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\uff1b\u9488\u5bf9\u56de\u5fc6\u751f\u6210\u4efb\u52a1\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b\u91c7\u7528\u5bf9\u6bd4\u8bad\u7ec3\u7b56\u7565\u5f00\u53d1\u591a\u6a21\u6001ToT\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u751f\u6210\u5f00\u653e\u5f0f\u8bb0\u5fc6\u63cf\u8ff0\u4e0a\u4f18\u4e8eGPT-4o\u7b49SOTA\u6a21\u578b\uff1b\u6210\u529f\u521b\u5efa\u9996\u4e2a\u652f\u6301\u591a\u6a21\u6001ToT\u68c0\u7d22\u7684\u6a21\u578b\uff0c\u8be5\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e30\u5bcc\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u6a21\u578b\u5f00\u8f9f\u89c6\u89c9\u5185\u5bb9\u8bb0\u5fc6\u6027\u7814\u7a76\u65b0\u65b9\u5411\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u8bb0\u5fc6\u7406\u89e3\u548c\u5185\u5bb9\u8bbe\u8ba1\u5e94\u7528\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.20691", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20691", "abs": "https://arxiv.org/abs/2511.20691", "authors": ["Lijun Shang", "Yadong Yu", "Wenqiang Kang", "Jian Zhou", "Dongyue Gao", "Pan Xiang", "Zhe Liu", "Mengyan Dai", "Zhonglu Guo", "Zhimei Sun"], "title": "LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data", "comment": "100 pages (18 pages main text, 82 pages supplementary material), 5 figures. Supplementary material starts from page 19", "summary": "Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe", "AI": {"tldr": "\u4e8c\u7ef4\uff082D\uff09\u6750\u6599\u56e0\u72ec\u7279\u7269\u7406\u5316\u5b66\u548c\u7535\u5b50\u6027\u80fd\u5728\u80fd\u6e90\u5b58\u50a8\u4e0e\u8f6c\u6362\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u5176\u5c5e\u6027\u548c\u5236\u5907\u4fe1\u606f\u4e3b\u8981\u6563\u5e03\u4e8e\u7814\u7a76\u8bba\u6587\u4e2d\uff0c\u4f46\u5408\u6210\u4fe1\u606f\u5206\u6563\u5bfc\u81f4\u83b7\u53d6\u56f0\u96be\u3002", "motivation": "\u89e3\u51b32D\u6750\u6599\u76f8\u5173\u5b9d\u8d35\u4fe1\u606f\uff08\u5982\u5c5e\u6027\u548c\u5236\u5907\u65b9\u6cd5\uff09\u6563\u5e03\u4e8e\u6d77\u91cf\u8bba\u6587\u4e2d\u3001\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u548c\u6574\u5408\u7684\u95ee\u9898\u3002", "method": "\u6458\u8981\u672a\u5b8c\u6574\u63d0\u4f9b\u5177\u4f53\u65b9\u6cd5\uff0c\u521d\u6b65\u63d0\u53ca\u6587\u732e\u4e2d\u4fe1\u606f\u7684\u5206\u6563\u6027\uff0c\u53ef\u80fd\u6d89\u53ca\u6587\u672c\u6316\u6398\u6216\u6570\u636e\u5e93\u6784\u5efa\uff08\u63a8\u6d4b\uff09\u3002", "result": "\u6458\u8981\u672a\u63d0\u4f9b\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u6458\u8981\u672a\u63d0\u4f9b\u7ed3\u8bba\u3002"}}
{"id": "2511.20865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20865", "abs": "https://arxiv.org/abs/2511.20865", "authors": ["Yining Ding", "Jo\u00e3o F. C. Mota", "Andrew M. Wallace", "Sen Wang"], "title": "Estimating Fog Parameters from a Sequence of Stereo Images", "comment": null, "summary": "We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.", "code_url": "https://github.com/SenseRoboticsLab/estimating-fog-parameters", "code_stars": 1, "code_last_update": "2025-11-25", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u7acb\u4f53\u96fe\u5929\u56fe\u50cf\u5e8f\u5217\u540c\u65f6\u4f30\u8ba1\u96fe\u6a21\u578b\u53c2\u6570\u5e76\u52a8\u6001\u66f4\u65b0\u7684\u65b9\u6cd5\uff0c\u4e0e\u987a\u5e8f\u4f30\u8ba1\u65b9\u6cd5\u4e0d\u540c\uff0c\u907f\u514d\u8bef\u5dee\u4f20\u64ad\u3002\u5047\u8bbe\u96fe\u5c40\u90e8\u5747\u5300\uff0c\u6709\u6548\u5904\u7406\u771f\u5b9e\u975e\u5747\u5300\u96fe\uff0c\u53ef\u96c6\u6210\u5230SLAM\u6216\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u4e2d\u3002\u65b0\u6570\u636e\u96c6SDIRF\u5305\u542b40\u5206\u949f34k\u5e27\u771f\u5b9e\u96fe\u5929\u9a7e\u9a76\u7acb\u4f53\u56fe\u50cf\u53ca\u6674\u5929\u5bf9\u5e94\u6570\u636e\u3002\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u987a\u5e8f\u4f30\u8ba1\u96fe\u53c2\u6570\u6613\u5bfc\u81f4\u8bef\u5dee\u4f20\u64ad\uff0c\u4e14\u5047\u8bbe\u96fe\u5168\u5c40\u5747\u5300\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u975e\u5747\u5300\u96fe\u3002\u96fe\u5929\u73af\u5883\u4e0b\u89c6\u89c9SLAM\u6216\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u9700\u8981\u51c6\u786e\u7684\u96fe\u53c2\u6570\u4f30\u8ba1\uff0c\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6c42\u89e3\u65b0\u578b\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u4f30\u8ba1\u6240\u6709\u96fe\u6a21\u578b\u53c2\u6570\uff08\u52a8\u6001\u66f4\u65b0\uff09\uff0c\u4ec5\u5047\u8bbe\u96fe\u5c40\u90e8\u5747\u5300\uff0c\u907f\u514d\u5168\u5c40\u5747\u5300\u5047\u8bbe\u3002\u6613\u4f5c\u4e3a\u63d2\u4ef6\u96c6\u6210\u5230\u73b0\u6709SLAM/\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u4e2d\u3002", "result": "\u5728\u5408\u6210\u96fe\u6570\u636e\u548c\u771f\u5b9eSDIRF\u6570\u636e\u96c6\u4e0a\uff0c\u53c2\u6570\u4f30\u8ba1\u6700\u51c6\u786e\uff0c\u9002\u5e94\u771f\u5b9e\u96fe\u6548\u679c\u66f4\u597d\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u96fe\u5929\u89c6\u89c9\u611f\u77e5\u6027\u80fd\uff0c\u63d0\u4f9b\u65b0\u6570\u636e\u96c6SDIRF\u548c\u4ee3\u7801\uff0c\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2511.20799", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20799", "abs": "https://arxiv.org/abs/2511.20799", "authors": ["Trung Cuong Dang", "David Mohaisen"], "title": "Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models", "comment": "11 pages, 2 tables, 8 figures", "summary": "Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u6613\u4e8e\u9010\u5b57\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u98ce\u9669\u3002\u73b0\u6709\u591a\u9879\u8bb0\u5fc6\u5b9a\u4e49\u5b58\u5728\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5bf9\u9f50\u6a21\u578b\u3002\u672c\u6587\u63d0\u51fa\u591a\u524d\u7f00\u8bb0\u5fc6\u6846\u67b6\uff1a\u8bb0\u5fc6\u5e8f\u5217\u53ef\u901a\u8fc7\u5927\u91cf\u4e0d\u540c\u524d\u7f00\u68c0\u7d22\u3002\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e3a\u5bf9\u6297\u641c\u7d22\u80fd\u627e\u5230\u76ee\u6807\u6570\u91cf\u4e0d\u540c\u524d\u7f00\u5f15\u53d1\u5e8f\u5217\u3002\u901a\u8fc7\u5f00\u6e90\u548c\u5bf9\u9f50\u804a\u5929\u6a21\u578b\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5176\u53ef\u9760\u533a\u5206\u8bb0\u5fc6\u4e0e\u975e\u8bb0\u5fc6\u6570\u636e\uff0c\u4e3a\u5ba1\u8ba1LLM\u6570\u636e\u6cc4\u6f0f\u63d0\u4f9b\u5de5\u5177\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e8e\u6d77\u91cf\u8bed\u6599\uff0c\u6613\u9010\u5b57\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u9020\u6210\u9690\u79c1\u548c\u7248\u6743\u98ce\u9669\u3002\u73b0\u6709\u8bb0\u5fc6\u5b9a\u4e49\u65e0\u6cd5\u5168\u9762\u6355\u6349\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u5bf9\u9f50\u6a21\u578b\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u9700\u8981\u65b0\u578b\u6846\u67b6\u5168\u9762\u8861\u91cf\u8bb0\u5fc6\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u591a\u524d\u7f00\u8bb0\u5fc6\u6846\u67b6\uff0c\u6838\u5fc3\u6d1e\u89c1\uff1a\u8bb0\u5fc6\u5e8f\u5217\u6df1\u5ea6\u7f16\u7801\uff0c\u53ef\u901a\u8fc7\u663e\u8457\u66f4\u591a\u4e0d\u540c\u524d\u7f00\u68c0\u7d22\u3002\u5f62\u5f0f\u5316\uff1a\u5e8f\u5217\u4e3a\u8bb0\u5fc6\uff0c\u82e5\u5916\u90e8\u5bf9\u6297\u641c\u7d22\u80fd\u8bc6\u522b\u76ee\u6807\u6570\u91cf\u4e0d\u540c\u524d\u7f00\u5f15\u53d1\u5b83\u3002\u5c06\u7126\u70b9\u4ece\u5355\u8def\u5f84\u63d0\u53d6\u8f6c\u5411\u91cf\u5316\u8bb0\u5fc6\u9c81\u68d2\u6027\uff0c\u5373\u68c0\u7d22\u8def\u5f84\u591a\u6837\u6027\u3002", "result": "\u5728\u5f00\u6e90\u6a21\u578b\u548c\u5bf9\u9f50\u804a\u5929\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u591a\u524d\u7f00\u5b9a\u4e49\u53ef\u9760\u533a\u5206\u8bb0\u5fc6\u4e0e\u975e\u8bb0\u5fc6\u6570\u636e\u3002", "conclusion": "\u591a\u524d\u7f00\u8bb0\u5fc6\u6846\u67b6\u4e3a\u5ba1\u8ba1LLM\u6570\u636e\u6cc4\u6f0f\u63d0\u4f9b\u9c81\u68d2\u3001\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2511.20886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20886", "abs": "https://arxiv.org/abs/2511.20886", "authors": ["Jiancheng Pan", "Runze Wang", "Tianwen Qian", "Mohammad Mahdi", "Yanwei Fu", "Xiangyang Xue", "Xiaomeng Huang", "Luc Van Gool", "Danda Pani Paudel", "Yuqian Fu"], "title": "V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence", "comment": "19 pages", "summary": "Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).", "AI": {"tldr": "V^2-SAM \u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u63d0\u793a\u751f\u6210\u5668\uff08V^2-Anchor \u548c V^2-Visual\uff09\u548c\u540e\u9a8c\u5faa\u73af\u4e00\u81f4\u6027\u9009\u62e9\u5668\uff08PCCS\uff09\uff0c\u5c06 SAM2 \u4ece\u5355\u89c6\u56fe\u5206\u5272\u9002\u914d\u5230\u8de8\u89c6\u56fe\u7269\u4f53\u5bf9\u5e94\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97 SOTA \u6027\u80fd\u3002", "motivation": "\u8de8\u89c6\u56fe\u7269\u4f53\u5bf9\u5e94\uff08\u5982\u81ea\u6211-\u5916\u90e8\u89c6\u56fe\uff09\u7531\u4e8e\u89c6\u89d2\u548c\u5916\u89c2\u5267\u53d8\u800c\u6781\u5177\u6311\u6218\uff0c\u73b0\u6210\u5206\u5272\u6a21\u578b\u5982 SAM2 \u96be\u4ee5\u76f4\u63a5\u5e94\u7528\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8de8\u89c6\u56fe\u6846\u67b6\u3002", "method": "V^2-SAM \u6846\u67b6\u5305\u62ec\uff1a\u57fa\u4e8e DINOv3 \u7279\u5f81\u7684\u8de8\u89c6\u56fe\u951a\u70b9\u63d0\u793a\u751f\u6210\u5668\uff08V^2-Anchor\uff09\uff0c\u5efa\u7acb\u51e0\u4f55\u611f\u77e5\u5bf9\u5e94\u5e76\u9996\u6b21\u542f\u7528 SAM2 \u7684\u5750\u6807\u63d0\u793a\uff1b\u8de8\u89c6\u56fe\u89c6\u89c9\u63d0\u793a\u751f\u6210\u5668\uff08V^2-Visual\uff09\uff0c\u901a\u8fc7\u65b0\u578b\u89c6\u89c9\u63d0\u793a\u5339\u914d\u5668\u4ece\u7279\u5f81\u548c\u7ed3\u6784\u89c6\u89d2\u589e\u5f3a\u5916\u89c2\u5f15\u5bfc\u3002\u591a\u4e13\u5bb6\u8bbe\u8ba1\u7ed3\u5408\u540e\u9a8c\u5faa\u73af\u4e00\u81f4\u6027\u9009\u62e9\u5668\uff08PCCS\uff09\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u4e13\u5bb6\u3002", "result": "\u5728 Ego-Exo4D\uff08\u81ea\u6211-\u5916\u90e8\u7269\u4f53\u5bf9\u5e94\uff09\u3001DAVIS-2017\uff08\u89c6\u9891\u7269\u4f53\u8ddf\u8e2a\uff09\u548c HANDAL-X\uff08\u673a\u5668\u4eba\u8de8\u89c6\u56fe\u5bf9\u5e94\uff09\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "V^2-SAM \u6709\u6548\u89e3\u51b3\u4e86\u8de8\u89c6\u56fe\u7269\u4f53\u5bf9\u5e94\u6311\u6218\uff0c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.20889", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20889", "abs": "https://arxiv.org/abs/2511.20889", "authors": ["Taehoon Kim", "Henry Gouk", "Timothy Hospedales"], "title": "Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation", "comment": null, "summary": "Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.", "AI": {"tldr": "\u63d0\u51faNull-Text Test-Time Alignment (Null-TTA)\uff0c\u901a\u8fc7\u4f18\u5316classifier-free guidance\u4e2d\u7684\u65e0\u6761\u4ef6\u5d4c\u5165\uff0c\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\uff0c\u907f\u514d\u6b20\u4f18\u5316\u6216\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u76f4\u63a5\u5f15\u5bfc\u751f\u6210\u5206\u5e03\u5411\u76ee\u6807\u5956\u52b1\u5bf9\u9f50\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u6613\u5bfc\u81f4\u6b20\u4f18\u5316\u6216\u8fc7\u4f18\u5316\uff08\u5956\u52b1\u9ed1\u5ba2\uff09\uff0c\u56e0\u5176\u64cd\u7eb5\u6f5c\u5728\u6216\u566a\u58f0\u53d8\u91cf\uff0c\u8131\u79bb\u8bed\u4e49\u6d41\u5f62\uff1b\u9700\u4e00\u79cd\u5728\u8bed\u4e49\u8fde\u8d2f\u6d41\u5f62\u4e0a\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002", "method": "\u4f18\u5316classifier-free guidance\u4e2d\u7684\u65e0\u6761\u4ef6\u5d4c\u5165\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u8bed\u4e49\u6027\u8d28\uff0c\u786e\u4fdd\u5bf9\u9f50\u53d1\u751f\u5728\u8bed\u4e49\u8fde\u8d2f\u6d41\u5f62\u4e0a\uff0c\u76f4\u63a5\u8f6c\u5411\u6a21\u578b\u751f\u6210\u5206\u5e03\u3002", "result": "\u5728\u76ee\u6807\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u8de8\u5956\u52b1\u6cdb\u5316\u3002", "conclusion": "\u786e\u7acb\u8bed\u4e49\u7a7a\u95f4\u4f18\u5316\u4f5c\u4e3aTTA\u7684\u6709\u6548\u4e14\u6709\u539f\u5219\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.20836", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20836", "abs": "https://arxiv.org/abs/2511.20836", "authors": ["Asad Aali", "Muhammad Ahmed Mohsin", "Vasiliki Bikia", "Arnav Singhvi", "Richard Gaus", "Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Yifan Mai", "Jordan Cahoon", "Michael Pfeffer", "Roxana Daneshjou", "Sanmi Koyejo", "Emily Alsentzer", "Percy Liang", "Christopher Potts", "Nigam H. Shah", "Akshay S. Chaudhari"], "title": "Structured Prompting Enables More Robust, Holistic Evaluation of Language Models", "comment": null, "summary": "As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller \u0394 across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).", "code_url": "https://github.com/stanford-crfm/helm", "code_stars": 2554, "code_last_update": "2025-11-23", "AI": {"tldr": "\u63d0\u51faDSPy+HELM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u53d1\u73b0HELM\u4f4e\u4f30LM\u6027\u80fd4%\uff0c\u5f00\u6e90\u6574\u5408\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5982HELM\u4f9d\u8d56\u56fa\u5b9a\u63d0\u793a\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540cLM\uff0c\u5bfc\u81f4\u6027\u80fd\u4f4e\u4f30\uff0c\u65e0\u6cd5\u51c6\u786e\u4f30\u8ba1LM\u4e0a\u9650\uff0c\u5f71\u54cd\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u53ef\u590d\u73b0DSPy+HELM\u6846\u67b6\uff0c\u5f15\u51654\u79cd\u7ed3\u6784\u5316\u63d0\u793a\uff08\u5305\u62ec\u94fe\u5f0f\u601d\u8003\uff09\uff0c\u57287\u4e2a\u901a\u7528/\u533b\u7597\u57fa\u51c6\u4e0a\u8bc4\u4f304\u4e2a\u524d\u6cbfLM\uff0c\u4e0eHELM\u57fa\u7ebf\u6bd4\u8f83\u3002", "result": "\u65e0\u7ed3\u6784\u63d0\u793a\u65f6\uff1a(i) HELM\u4f4e\u4f30\u6027\u80fd4%\uff1b(ii) \u57fa\u51c6\u95f4\u53d8\u5f02\u589e\u52a02%\u6807\u51c6\u5dee\uff1b(iii) \u6392\u884c\u57283/7\u57fa\u51c6\u7ffb\u8f6c\uff1b(iv) \u63a8\u7406\u63d0\u793a\u964d\u4f4eLM\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\u6027\u3002", "conclusion": "\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u523b\u753bLM\u5728\u57fa\u51c6\u4e0e\u63d0\u793a\u65b9\u6cd5\u95f4\u7684\u884c\u4e3a\uff0c\u8bc1\u660e\u53ef\u6269\u5c55\u4e0a\u9650\u4f30\u8ba1\u63d0\u4f9b\u66f4\u53ef\u9760\u57fa\u51c6\uff0c\u5e76\u5f00\u6e90DSPy+HELM\u6574\u5408\u4e0e\u63d0\u793a\u4f18\u5316\u7ba1\u9053\u3002"}}
{"id": "2511.20849", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20849", "abs": "https://arxiv.org/abs/2511.20849", "authors": ["Dong Dong", "Weijie Su"], "title": "Length-MAX Tokenizer for Language Models", "comment": null, "summary": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference.", "AI": {"tldr": "\u63d0\u51faLength-MAX\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5e73\u5747\u6bcf\u5b57\u7b26token\u6570\uff0c\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684token\u6570\u91cf\uff0c\u6bd4BPE\u51cf\u5c1114-18% token\uff0c\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f\u3001\u4e0b\u6e38\u4efb\u52a1\u63d0\u5347\u548c\u63a8\u7406\u6548\u7387\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u5206\u8bcd\u5668\u5982BPE\u4ec5\u57fa\u4e8e\u9891\u7387\u4f18\u5316\uff0c\u672a\u8003\u8651token\u957f\u5ea6\uff0c\u5bfc\u81f4\u5e73\u5747token\u8f83\u957f\uff0c\u589e\u52a0token\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\uff1b\u52a8\u673a\u662f\u901a\u8fc7\u957f\u5ea6\u52a0\u6743\u4f18\u5316\uff0c\u7f29\u77ed\u5e73\u5747token\u957f\u5ea6\u4ee5\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u6548\u7387\u3002", "method": "\u5c06\u957f\u5ea6\u52a0\u6743\u76ee\u6807\u6700\u5927\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u56fe\u5206\u5272\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u8d2a\u5a6a\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ece10K\u81f364K\u8bcd\u6c47\u89c4\u6a21\u6784\u5efa\u8bcd\u6c47\u8868\u3002", "result": "\u5728FineWeb\u53ca\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0c\u51cf\u5c1114-18% token\uff0864K\u65f613%\uff09\uff1b\u4ece\u5934\u8bad\u7ec3GPT-2\uff08124M/355M/1.3B\uff09\u9700\u5c1118.5%/17.2%/18.5%\u6b65\u8fbe\u56fa\u5b9a\u635f\u5931\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d13.7%/12.7%/13.7%\uff0c124M\u65f6\u541e\u5410\u589e16%\uff1b\u4e0b\u6e38LAMBADA\u56f0\u60d1\u5ea6\u964d11.7%\uff0cHellaSwag\u51c6\u786e\u7387\u53474.3%\uff1b\u8bcd\u6c47\u8986\u76d699.62%\uff0cOOV\u73870.12%\uff1b\u63a8\u7406\u5d4c\u5165\u548cKV-cache\u5185\u5b58\u964d18%\u3002", "conclusion": "\u4f18\u5316\u5e73\u5747token\u957f\u5ea6\u800c\u975e\u4ec5\u9891\u7387\uff0c\u662f\u9ad8\u6548\u8bed\u8a00\u5efa\u6a21\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e0d\u727a\u7272\u4e14\u5e38\u6539\u5584\u4e0b\u6e38\u6027\u80fd\uff1b\u517c\u5bb9\u751f\u4ea7\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u5ef6\u8fdf\u3002"}}
